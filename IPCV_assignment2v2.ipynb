{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# Assignment Module 2: Product Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTQUJ4uYH1w"
      },
      "source": [
        "## Preliminaries: the dataset\n",
        "\n",
        "The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "The products belong to the following 43 classes:\n",
        "```\n",
        "0.  Apple\n",
        "1.  Avocado\n",
        "2.  Banana\n",
        "3.  Kiwi\n",
        "4.  Lemon\n",
        "5.  Lime\n",
        "6.  Mango\n",
        "7.  Melon\n",
        "8.  Nectarine\n",
        "9.  Orange\n",
        "10. Papaya\n",
        "11. Passion-Fruit\n",
        "12. Peach\n",
        "13. Pear\n",
        "14. Pineapple\n",
        "15. Plum\n",
        "16. Pomegranate\n",
        "17. Red-Grapefruit\n",
        "18. Satsumas\n",
        "19. Juice\n",
        "20. Milk\n",
        "21. Oatghurt\n",
        "22. Oat-Milk\n",
        "23. Sour-Cream\n",
        "24. Sour-Milk\n",
        "25. Soyghurt\n",
        "26. Soy-Milk\n",
        "27. Yoghurt\n",
        "28. Asparagus\n",
        "29. Aubergine\n",
        "30. Cabbage\n",
        "31. Carrots\n",
        "32. Cucumber\n",
        "33. Garlic\n",
        "34. Ginger\n",
        "35. Leek\n",
        "36. Mushroom\n",
        "37. Onion\n",
        "38. Pepper\n",
        "39. Potato\n",
        "40. Red-Beet\n",
        "41. Tomato\n",
        "42. Zucchini\n",
        "```\n",
        "\n",
        "The dataset is split into training (`train`), validation (`val`), and test (`test`) set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pdrmJRnJPd8"
      },
      "source": [
        "The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:01.404011Z",
          "iopub.status.busy": "2024-07-06T07:17:01.403009Z",
          "iopub.status.idle": "2024-07-06T07:17:12.053264Z",
          "shell.execute_reply": "2024-07-06T07:17:12.052332Z",
          "shell.execute_reply.started": "2024-07-06T07:17:01.403976Z"
        },
        "id": "POMX_3x-_bZI",
        "outputId": "044d8916-cdab-4182-d9ca-511257fe834d",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroceryStoreDataset'...\n",
            "remote: Enumerating objects: 6559, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293\u001b[K\n",
            "Receiving objects: 100% (6559/6559), 116.26 MiB | 15.22 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:12.056104Z",
          "iopub.status.busy": "2024-07-06T07:17:12.055639Z",
          "iopub.status.idle": "2024-07-06T07:17:16.806954Z",
          "shell.execute_reply": "2024-07-06T07:17:16.806008Z",
          "shell.execute_reply.started": "2024-07-06T07:17:12.056066Z"
        },
        "id": "Q4bU2LAr_LPh",
        "outputId": "febae809-5153-4e35-ebc2-a0705132ca55",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor,device\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:16.808755Z",
          "iopub.status.busy": "2024-07-06T07:17:16.808253Z",
          "iopub.status.idle": "2024-07-06T07:17:16.818672Z",
          "shell.execute_reply": "2024-07-06T07:17:16.817790Z",
          "shell.execute_reply.started": "2024-07-06T07:17:16.808723Z"
        },
        "id": "yWGsm5z5qos9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "fix_random(45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:16.821881Z",
          "iopub.status.busy": "2024-07-06T07:17:16.821126Z",
          "iopub.status.idle": "2024-07-06T07:17:16.831074Z",
          "shell.execute_reply": "2024-07-06T07:17:16.830071Z",
          "shell.execute_reply.started": "2024-07-06T07:17:16.821856Z"
        },
        "id": "jROSO2qVDxdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GroceryStoreDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = Path(\"GroceryStoreDataset/dataset\")\n",
        "        self.split = split\n",
        "        self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                # path, fine-grained class, coarse-grained class\n",
        "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
        "                paths.append(path), labels.append(int(label))\n",
        "\n",
        "        return paths, labels\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:16.832446Z",
          "iopub.status.busy": "2024-07-06T07:17:16.832177Z",
          "iopub.status.idle": "2024-07-06T07:17:16.854453Z",
          "shell.execute_reply": "2024-07-06T07:17:16.853375Z",
          "shell.execute_reply.started": "2024-07-06T07:17:16.832424Z"
        },
        "id": "pNnuPzhOzj0s",
        "outputId": "7970284e-89f2-4894-d29b-426ae622569e",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2640, 296, 2485)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "train_dataset = GroceryStoreDataset(split='train',transform=transform)\n",
        "val_dataset = GroceryStoreDataset(split='val',transform=transform)\n",
        "test_dataset = GroceryStoreDataset(split='test',transform=transform)\n",
        "train_dataset.__len__(),val_dataset.__len__(),test_dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:16.856173Z",
          "iopub.status.busy": "2024-07-06T07:17:16.855579Z",
          "iopub.status.idle": "2024-07-06T07:17:20.962102Z",
          "shell.execute_reply": "2024-07-06T07:17:20.961147Z",
          "shell.execute_reply.started": "2024-07-06T07:17:16.856125Z"
        },
        "id": "ASJ8mG9Q074x",
        "outputId": "eb7f340c-ee19-412e-e24a-8cc81eb18a92",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(464, 348)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def check_image_shapes(dataset):\n",
        "    for i in range(dataset.__len__()):\n",
        "        img, label = dataset[i]\n",
        "        w, h = 0,0\n",
        "        #print(f\"Image {i} shape: {img.shape} (width, height)\")\n",
        "        if img.shape[1] > w:\n",
        "          w = img.shape[1]\n",
        "        if img.shape[2] > h:\n",
        "          h = img.shape[2]\n",
        "    return w,h\n",
        "# Check shapes of the first few images\n",
        "w_t,h_t = check_image_shapes(train_dataset)\n",
        "w_t,h_t\n",
        "#w_v,h_v = check_image_shapes(val_dataset)\n",
        "#w_T,h_T = check_image_shapes(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:20.964029Z",
          "iopub.status.busy": "2024-07-06T07:17:20.963686Z",
          "iopub.status.idle": "2024-07-06T07:17:29.869670Z",
          "shell.execute_reply": "2024-07-06T07:17:29.868755Z",
          "shell.execute_reply.started": "2024-07-06T07:17:20.963998Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNn9VRNes1vU",
        "outputId": "b6c87b58-29ab-4cbd-a3f3-f7c4b0204685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: tensor([0.5306, 0.3964, 0.2564])\n",
            "Std: tensor([0.2325, 0.2093, 0.1781])\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "image_height, image_width, image_channels = 348,348,3\n",
        "# Define the transform to resize images\n",
        "resize_transform = transforms.Compose([\n",
        "    transforms.Resize((image_height, image_width)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the training dataset with the resizing transform\n",
        "train_dataset = GroceryStoreDataset(split='train',transform=resize_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize variables to store the mean and std\n",
        "mean = torch.zeros(3)\n",
        "std = torch.zeros(3)\n",
        "nb_samples = 0\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "for images, _ in train_loader:\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std += images.std(2).sum(0)\n",
        "    nb_samples += batch_samples\n",
        "\n",
        "mean /= nb_samples\n",
        "std /= nb_samples\n",
        "\n",
        "print(f'Mean: {mean}')\n",
        "print(f'Std: {std}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBch3dpwNSsW"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n",
        "\n",
        "- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:39.940424Z",
          "iopub.status.busy": "2024-07-06T07:17:39.940032Z",
          "iopub.status.idle": "2024-07-06T07:17:39.955426Z",
          "shell.execute_reply": "2024-07-06T07:17:39.954483Z",
          "shell.execute_reply.started": "2024-07-06T07:17:39.940389Z"
        },
        "id": "SFKZXhGDAqto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def ncorrect(scores, y):\n",
        "    y_hat = torch.argmax(scores, -1)\n",
        "    return (y_hat == y).sum()\n",
        "\n",
        "def accuracy(scores, y):\n",
        "    correct = ncorrect(scores, y)\n",
        "    return correct.true_divide(y.shape[0])\n",
        "\n",
        "def train_loop(model, train_dl, epochs, opt, val_dl=None, verbose=False, label_smoothing=0):\n",
        "    best_val_acc = 0\n",
        "    best_params = []\n",
        "    best_epoch = -1\n",
        "    found = False\n",
        "    for e in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        # Train\n",
        "        train_loss = 0\n",
        "        train_samples = 0\n",
        "        train_acc = 0\n",
        "        for train_data in train_dl:\n",
        "            imgs = train_data[0].to(device)\n",
        "            labels = train_data[1].to(device)\n",
        "            scores = model(imgs)\n",
        "            loss = F.cross_entropy(scores, labels,label_smoothing=label_smoothing)\n",
        "            train_loss += loss.item() * imgs.shape[0]\n",
        "            train_samples += imgs.shape[0]\n",
        "            train_acc += ncorrect(scores, labels).item()\n",
        "\n",
        "            opt.zero_grad()  # clear\n",
        "            loss.backward()  # fill\n",
        "            opt.step()       # use\n",
        "\n",
        "        train_acc /= train_samples\n",
        "        train_loss /= train_samples\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            val_samples = 0\n",
        "            val_acc = 0\n",
        "            if val_dl is not None:\n",
        "                for val_data in val_dl:\n",
        "                    imgs = val_data[0].to(device)\n",
        "                    labels = val_data[1].to(device)\n",
        "                    val_scores = model(imgs)\n",
        "                    val_loss += F.cross_entropy(val_scores, labels).item() * imgs.shape[0]\n",
        "                    val_samples += imgs.shape[0]\n",
        "                    val_acc += ncorrect(val_scores, labels).item()\n",
        "                val_acc /= val_samples\n",
        "                val_loss /= val_samples\n",
        "\n",
        "            if val_dl is None or val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc if val_dl is not None else 0\n",
        "                best_params = model.state_dict()\n",
        "                torch.save(best_params, \"best_model.pth\")\n",
        "                best_epoch = e\n",
        "\n",
        "        if verbose: #and e % 5 == 0:\n",
        "            print(f\"Epoch {e}: train loss {train_loss:.3f} - train acc {train_acc:.3f}\" + (\"\" if val_dl is None else f\" - valid loss {val_loss:.3f} - valid acc {val_acc:.3f}\"))\n",
        "            if val_acc >= 0.60:\n",
        "                best_params = model.state_dict()\n",
        "                torch.save(best_params, \"A2.pth\")\n",
        "                best_epoch = e\n",
        "                break\n",
        "\n",
        "    if verbose and val_dl is not None:\n",
        "        print(f\"Best epoch {best_epoch}, best acc {best_val_acc}\")\n",
        "\n",
        "    return (best_val_acc, best_params, best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RNVD7TQoy1r"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T09:29:23.191008Z",
          "iopub.status.busy": "2024-07-06T09:29:23.190418Z",
          "iopub.status.idle": "2024-07-06T09:29:23.205355Z",
          "shell.execute_reply": "2024-07-06T09:29:23.204494Z",
          "shell.execute_reply.started": "2024-07-06T09:29:23.190980Z"
        },
        "id": "kVYlsNGkbuqb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_height, image_width, image_channels = 348,348,3 #224, 224, 3\n",
        "num_classes = train_dataset.get_num_classes()\n",
        "\n",
        "transform_2 = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomResizedCrop(size=(image_height, image_width),antialias=True),\n",
        "    #torchvision.transforms.Resize((image_height, image_width)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean, std)\n",
        "    #torchvision.transforms.Normalize((0.5, 0.4, 0.25), (0.23, 0.2, 0.17))\n",
        "])\n",
        "\n",
        "val_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((image_height, image_width)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean, std)\n",
        "    #torchvision.transforms.transforms.Normalize((0.5, 0.4, 0.25), (0.23, 0.2, 0.17))\n",
        "])\n",
        "train_dataset = GroceryStoreDataset(split='train',transform=transform_2)\n",
        "val_dataset = GroceryStoreDataset(split='val',transform=val_transforms)\n",
        "#test_dataset = GroceryStoreDataset(split='test',transform=transform_2)\n",
        "\n",
        "batch = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:17:39.977526Z",
          "iopub.status.busy": "2024-07-06T07:17:39.977269Z",
          "iopub.status.idle": "2024-07-06T07:17:39.992090Z",
          "shell.execute_reply": "2024-07-06T07:17:39.991319Z",
          "shell.execute_reply.started": "2024-07-06T07:17:39.977504Z"
        },
        "id": "CZNxdSS2o3oT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self,channels=16, num_classes=num_classes,p_dropout=0):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels*2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=channels*2, out_channels=channels*4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=channels*4, out_channels=channels*8, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Calculate the size of the feature map after the convolutional layers\n",
        "        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n",
        "            return (size - kernel_size + 2 * padding) // stride + 1\n",
        "        def conv2d_compute_size(conv_number,kernels,max_poolings):  #max pooling ex. [1,0,1] if 1 them there is a max_pool\n",
        "          image_w = image_width\n",
        "          for i in range(conv_number):\n",
        "            image_w = conv2d_size_out(image_w, kernel_size=kernels[i])\n",
        "            if max_poolings[i] == 1:\n",
        "              image_w = image_w//2\n",
        "          return image_w\n",
        "\n",
        "        #convh = conv2d_size_out(conv2d_size_out(image_height, kernel_size=5, padding=1) // 2, kernel_size=3, padding=1) // 2\n",
        "        image_w = conv2d_compute_size(4,[3,3,3,3],[1,1,1,1])\n",
        "\n",
        "        linear_input_size = image_w * image_w * channels*8  # Adjusted based on the output channels of conv2\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(linear_input_size, 128)\n",
        "        #self.drop1 = nn.Dropout(p_dropout)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        #self.drop2 = nn.Dropout(p_dropout)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with ReLU activation and max pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten the tensor\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.drop1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x = self.drop2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761,
          "referenced_widgets": [
            "521788e7ab614c4bb1ec45d9853678db",
            "d18c07c8179043df9447c93111a31dd0",
            "4733ff38e6f54f72a8dca7b6d1d6e1f5",
            "71a09f42d1f34c5faf7c012fbfe5fc1e",
            "f7ded3abfb644ed3aaa967d1bc313eb6",
            "febae8e0d3764652bbf57351fbfb5041",
            "7b39c4ee962d4b019efe256eb5bc4b42",
            "7cd9473ac34347508ec6f23534b463b2",
            "0d21484dba744589823686ea95ce27f8",
            "6a5d0a75de624cc5a80388acd0cafad1",
            "84f66999ecb34dd284cbb5b1961e009c",
            "8fdc9c5e3ca74bb7ad60541db17b41d5"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-07-06T09:29:30.155863Z",
          "iopub.status.busy": "2024-07-06T09:29:30.155521Z"
        },
        "id": "TpE8SRf5ptIM",
        "outputId": "53c4d936-42b1-4d1e-eccd-3435a1820580",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fdc9c5e3ca74bb7ad60541db17b41d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: train loss 3.243 - train acc 0.162 - valid loss 3.005 - valid acc 0.186\n",
            "Epoch 1: train loss 2.826 - train acc 0.243 - valid loss 2.623 - valid acc 0.277\n",
            "Epoch 2: train loss 2.589 - train acc 0.311 - valid loss 2.867 - valid acc 0.209\n",
            "Epoch 3: train loss 2.363 - train acc 0.385 - valid loss 2.228 - valid acc 0.294\n",
            "Epoch 4: train loss 2.179 - train acc 0.447 - valid loss 2.199 - valid acc 0.304\n",
            "Epoch 5: train loss 2.089 - train acc 0.482 - valid loss 1.878 - valid acc 0.453\n",
            "Epoch 6: train loss 1.953 - train acc 0.520 - valid loss 2.026 - valid acc 0.389\n",
            "Epoch 7: train loss 1.878 - train acc 0.556 - valid loss 2.066 - valid acc 0.382\n",
            "Epoch 8: train loss 1.832 - train acc 0.569 - valid loss 1.845 - valid acc 0.432\n",
            "Epoch 9: train loss 1.753 - train acc 0.599 - valid loss 1.900 - valid acc 0.429\n",
            "Epoch 10: train loss 1.714 - train acc 0.616 - valid loss 1.720 - valid acc 0.439\n",
            "Epoch 11: train loss 1.665 - train acc 0.639 - valid loss 2.097 - valid acc 0.446\n",
            "Epoch 12: train loss 1.628 - train acc 0.660 - valid loss 1.860 - valid acc 0.459\n",
            "Epoch 13: train loss 1.585 - train acc 0.669 - valid loss 1.751 - valid acc 0.483\n",
            "Epoch 14: train loss 1.582 - train acc 0.674 - valid loss 1.931 - valid acc 0.443\n",
            "Epoch 15: train loss 1.558 - train acc 0.683 - valid loss 1.677 - valid acc 0.503\n",
            "Epoch 16: train loss 1.461 - train acc 0.726 - valid loss 1.889 - valid acc 0.493\n",
            "Epoch 17: train loss 1.469 - train acc 0.716 - valid loss 1.880 - valid acc 0.493\n",
            "Epoch 18: train loss 1.469 - train acc 0.727 - valid loss 1.833 - valid acc 0.493\n",
            "Epoch 19: train loss 1.411 - train acc 0.751 - valid loss 1.969 - valid acc 0.490\n",
            "Epoch 20: train loss 1.388 - train acc 0.767 - valid loss 1.869 - valid acc 0.490\n",
            "Epoch 21: train loss 1.406 - train acc 0.762 - valid loss 1.882 - valid acc 0.466\n",
            "Epoch 22: train loss 1.363 - train acc 0.770 - valid loss 1.826 - valid acc 0.490\n",
            "Epoch 23: train loss 1.329 - train acc 0.786 - valid loss 1.740 - valid acc 0.520\n",
            "Epoch 24: train loss 1.317 - train acc 0.787 - valid loss 1.987 - valid acc 0.456\n",
            "Epoch 25: train loss 1.301 - train acc 0.801 - valid loss 1.658 - valid acc 0.537\n",
            "Epoch 26: train loss 1.272 - train acc 0.813 - valid loss 1.845 - valid acc 0.503\n",
            "Epoch 27: train loss 1.273 - train acc 0.819 - valid loss 2.028 - valid acc 0.443\n",
            "Epoch 28: train loss 1.244 - train acc 0.826 - valid loss 1.838 - valid acc 0.541\n",
            "Epoch 29: train loss 1.283 - train acc 0.810 - valid loss 1.820 - valid acc 0.507\n",
            "Epoch 30: train loss 1.264 - train acc 0.816 - valid loss 1.963 - valid acc 0.483\n",
            "Epoch 31: train loss 1.249 - train acc 0.822 - valid loss 1.863 - valid acc 0.480\n",
            "Epoch 32: train loss 1.198 - train acc 0.836 - valid loss 1.681 - valid acc 0.544\n",
            "Epoch 33: train loss 1.232 - train acc 0.826 - valid loss 1.534 - valid acc 0.581\n",
            "Epoch 34: train loss 1.210 - train acc 0.835 - valid loss 1.874 - valid acc 0.497\n",
            "Epoch 35: train loss 1.192 - train acc 0.839 - valid loss 1.754 - valid acc 0.541\n",
            "Epoch 36: train loss 1.198 - train acc 0.840 - valid loss 1.916 - valid acc 0.446\n",
            "Epoch 37: train loss 1.175 - train acc 0.845 - valid loss 1.720 - valid acc 0.507\n",
            "Epoch 38: train loss 1.189 - train acc 0.842 - valid loss 1.702 - valid acc 0.473\n",
            "Epoch 39: train loss 1.155 - train acc 0.858 - valid loss 1.763 - valid acc 0.510\n",
            "Epoch 40: train loss 1.129 - train acc 0.870 - valid loss 2.118 - valid acc 0.432\n",
            "Epoch 41: train loss 1.138 - train acc 0.868 - valid loss 1.753 - valid acc 0.517\n",
            "Epoch 42: train loss 1.133 - train acc 0.873 - valid loss 1.736 - valid acc 0.544\n",
            "Epoch 43: train loss 1.120 - train acc 0.875 - valid loss 1.799 - valid acc 0.517\n",
            "Epoch 44: train loss 1.147 - train acc 0.858 - valid loss 1.890 - valid acc 0.503\n",
            "Epoch 45: train loss 1.192 - train acc 0.839 - valid loss 1.556 - valid acc 0.574\n",
            "Epoch 46: train loss 1.119 - train acc 0.867 - valid loss 1.873 - valid acc 0.514\n",
            "Epoch 47: train loss 1.133 - train acc 0.877 - valid loss 1.694 - valid acc 0.507\n",
            "Epoch 48: train loss 1.140 - train acc 0.863 - valid loss 1.551 - valid acc 0.598\n",
            "Epoch 49: train loss 1.126 - train acc 0.864 - valid loss 1.610 - valid acc 0.520\n",
            "Epoch 50: train loss 1.128 - train acc 0.865 - valid loss 1.741 - valid acc 0.517\n",
            "Epoch 51: train loss 1.135 - train acc 0.863 - valid loss 1.773 - valid acc 0.520\n",
            "Epoch 52: train loss 1.097 - train acc 0.884 - valid loss 1.637 - valid acc 0.551\n",
            "Epoch 53: train loss 1.097 - train acc 0.879 - valid loss 1.641 - valid acc 0.507\n",
            "Epoch 54: train loss 1.123 - train acc 0.870 - valid loss 1.727 - valid acc 0.561\n",
            "Epoch 55: train loss 1.115 - train acc 0.873 - valid loss 1.640 - valid acc 0.581\n",
            "Epoch 56: train loss 1.096 - train acc 0.883 - valid loss 1.919 - valid acc 0.507\n",
            "Epoch 57: train loss 1.103 - train acc 0.880 - valid loss 1.982 - valid acc 0.476\n",
            "Epoch 58: train loss 1.070 - train acc 0.891 - valid loss 1.771 - valid acc 0.551\n",
            "Epoch 59: train loss 1.104 - train acc 0.874 - valid loss 1.737 - valid acc 0.554\n",
            "Epoch 60: train loss 1.090 - train acc 0.881 - valid loss 1.748 - valid acc 0.534\n",
            "Epoch 61: train loss 1.112 - train acc 0.874 - valid loss 1.683 - valid acc 0.541\n",
            "Epoch 62: train loss 1.091 - train acc 0.889 - valid loss 1.780 - valid acc 0.551\n",
            "Epoch 63: train loss 1.082 - train acc 0.884 - valid loss 1.608 - valid acc 0.578\n",
            "Epoch 64: train loss 1.084 - train acc 0.880 - valid loss 1.831 - valid acc 0.507\n",
            "Epoch 65: train loss 1.070 - train acc 0.891 - valid loss 1.756 - valid acc 0.534\n",
            "Epoch 66: train loss 1.071 - train acc 0.889 - valid loss 1.637 - valid acc 0.557\n",
            "Epoch 67: train loss 1.080 - train acc 0.886 - valid loss 1.443 - valid acc 0.588\n",
            "Epoch 68: train loss 1.046 - train acc 0.898 - valid loss 1.667 - valid acc 0.571\n",
            "Epoch 69: train loss 1.049 - train acc 0.900 - valid loss 1.531 - valid acc 0.598\n",
            "Epoch 70: train loss 1.066 - train acc 0.890 - valid loss 1.805 - valid acc 0.517\n",
            "Epoch 71: train loss 1.076 - train acc 0.886 - valid loss 1.737 - valid acc 0.534\n",
            "Epoch 72: train loss 1.068 - train acc 0.892 - valid loss 1.524 - valid acc 0.615\n",
            "Best epoch 72, best acc 0.6148648648648649\n"
          ]
        }
      ],
      "source": [
        "new_model = ExtendedCNN(\n",
        "    num_classes = num_classes,\n",
        ")\n",
        "new_model.to(device)\n",
        "\n",
        "#Training with: weight_decay=0.0, label_smoothing=0.5, lr=0.001, channels=16, batch_size=8\n",
        "\n",
        "#optimizer = torch.optim.AdamW(new_model.parameters(), lr=0.001,weight_decay=0.01)\n",
        "optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 80\n",
        "label_smoothing = 0.1\n",
        "best_val_acc, best_params, best_epoch = train_loop(\n",
        "    new_model,\n",
        "    train_loader,\n",
        "    epochs,\n",
        "    optimizer,\n",
        "    val_loader,\n",
        "    verbose=True,\n",
        "    label_smoothing = label_smoothing,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "new_model = ExtendedCNN(\n",
        "    num_classes = num_classes,\n",
        ")\n",
        "new_model.to(device)\n",
        "\n",
        "#Training with: weight_decay=0.0, label_smoothing=0.5, lr=0.001, channels=16, batch_size=8\n",
        "\n",
        "#optimizer = torch.optim.AdamW(new_model.parameters(), lr=0.001,weight_decay=0.01)\n",
        "optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001)\n",
        "\n",
        "# Chemin vers le fichier de poids sauvegardé\n",
        "model_path = \"/content/drive/MyDrive/AssignmentsIPCV/A2.pth\"\n",
        "\n",
        "# Charger les poids sauvegardés\n",
        "new_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Assurez-vous que le modèle est en mode évaluation\n",
        "new_model.eval()\n",
        "\n",
        "# Si vous avez besoin de tester le modèle ou de faire des prédictions, utilisez-le comme suit\n",
        "# Exemple: prédiction sur des données de validation\n",
        "with torch.no_grad():\n",
        "    for val_data in val_loader:\n",
        "        imgs = val_data[0].to(device)\n",
        "        labels = val_data[1].to(device)\n",
        "        outputs = new_model(imgs)\n",
        "        # Traitez les outputs selon vos besoins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxkMt_-zvwIf",
        "outputId": "43059d9b-0cd3-48ab-ecd3-a64b6ee4136c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsEgFa94s1vV"
      },
      "source": [
        "# Gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# # Montez Google Drive si nécessaire\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Assurez-vous que le dataset GroceryStoreDataset est disponible et bien structuré\n",
        "# Utilisez la même transformation que celle utilisée pour le modèle précédent\n",
        "transform_2 = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(image_height, image_width), antialias=True),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_height, image_width)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_dataset = GroceryStoreDataset(split='train', transform=transform_2)\n",
        "val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the pretrained model\n",
        "resnet18 = torchvision.models.resnet18(weights=True)\n",
        "\n",
        "# adapt to the dataset GroceryStoreDataset\n",
        "num_classes = train_dataset.get_num_classes()\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# to use the gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet18 = resnet18.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wwsNlNXtlAB",
        "outputId": "c36749e4-0063-43d2-8951-bc26b008426f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparamater of Part A\n",
        "learning_rate = 0.001\n",
        "epochs = 80\n",
        "label_smoothing = 0.1\n",
        "\n",
        "# Définir l'optimiseur et la fonction de perte\n",
        "optimizer = optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "\n",
        "# Function to train the model\n",
        "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_acc = train_correct / total_train\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = val_correct / total_val\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "            torch.save(best_model_wts, \"resnet18_best_model.pth\")\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/total_train:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss/total_val:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Training\n",
        "best_model = train_and_validate(resnet18, train_loader, val_loader, criterion, optimizer, epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4u4ojvLyVom",
        "outputId": "e644d520-cd31-4141-ef22-81edca4f26d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Train Loss: 0.3831, Train Acc: 0.2367, Val Loss: 0.4182, Val Acc: 0.1926\n",
            "Epoch [2/80], Train Loss: 0.3249, Train Acc: 0.3398, Val Loss: 0.3104, Val Acc: 0.3581\n",
            "Epoch [3/80], Train Loss: 0.2940, Train Acc: 0.4170, Val Loss: 0.3269, Val Acc: 0.4595\n",
            "Epoch [4/80], Train Loss: 0.2599, Train Acc: 0.5216, Val Loss: 0.3112, Val Acc: 0.4730\n",
            "Epoch [5/80], Train Loss: 0.2412, Train Acc: 0.5667, Val Loss: 0.2846, Val Acc: 0.4966\n",
            "Epoch [6/80], Train Loss: 0.2152, Train Acc: 0.6489, Val Loss: 0.2546, Val Acc: 0.5709\n",
            "Epoch [7/80], Train Loss: 0.2091, Train Acc: 0.6682, Val Loss: 0.2653, Val Acc: 0.5811\n",
            "Epoch [8/80], Train Loss: 0.1913, Train Acc: 0.7136, Val Loss: 0.2373, Val Acc: 0.6216\n",
            "Epoch [9/80], Train Loss: 0.1895, Train Acc: 0.7333, Val Loss: 0.2517, Val Acc: 0.5777\n",
            "Epoch [10/80], Train Loss: 0.1789, Train Acc: 0.7689, Val Loss: 0.2357, Val Acc: 0.6047\n",
            "Epoch [11/80], Train Loss: 0.1696, Train Acc: 0.7909, Val Loss: 0.2243, Val Acc: 0.6318\n",
            "Epoch [12/80], Train Loss: 0.1652, Train Acc: 0.8098, Val Loss: 0.2585, Val Acc: 0.5709\n",
            "Epoch [13/80], Train Loss: 0.1631, Train Acc: 0.8208, Val Loss: 0.2318, Val Acc: 0.6250\n",
            "Epoch [14/80], Train Loss: 0.1591, Train Acc: 0.8341, Val Loss: 0.2164, Val Acc: 0.7297\n",
            "Epoch [15/80], Train Loss: 0.1539, Train Acc: 0.8527, Val Loss: 0.2224, Val Acc: 0.6926\n",
            "Epoch [16/80], Train Loss: 0.1490, Train Acc: 0.8697, Val Loss: 0.2244, Val Acc: 0.6791\n",
            "Epoch [17/80], Train Loss: 0.1437, Train Acc: 0.8803, Val Loss: 0.2035, Val Acc: 0.7264\n",
            "Epoch [18/80], Train Loss: 0.1469, Train Acc: 0.8731, Val Loss: 0.2246, Val Acc: 0.6622\n",
            "Epoch [19/80], Train Loss: 0.1433, Train Acc: 0.8822, Val Loss: 0.2103, Val Acc: 0.6959\n",
            "Epoch [20/80], Train Loss: 0.1416, Train Acc: 0.8792, Val Loss: 0.1917, Val Acc: 0.7128\n",
            "Epoch [21/80], Train Loss: 0.1353, Train Acc: 0.9030, Val Loss: 0.2187, Val Acc: 0.6993\n",
            "Epoch [22/80], Train Loss: 0.1387, Train Acc: 0.8845, Val Loss: 0.2059, Val Acc: 0.6824\n",
            "Epoch [23/80], Train Loss: 0.1330, Train Acc: 0.9087, Val Loss: 0.2044, Val Acc: 0.7162\n",
            "Epoch [24/80], Train Loss: 0.1322, Train Acc: 0.9144, Val Loss: 0.1865, Val Acc: 0.7432\n",
            "Epoch [25/80], Train Loss: 0.1291, Train Acc: 0.9223, Val Loss: 0.2015, Val Acc: 0.7264\n",
            "Epoch [26/80], Train Loss: 0.1267, Train Acc: 0.9314, Val Loss: 0.1961, Val Acc: 0.7365\n",
            "Epoch [27/80], Train Loss: 0.1288, Train Acc: 0.9220, Val Loss: 0.1871, Val Acc: 0.7736\n",
            "Epoch [28/80], Train Loss: 0.1282, Train Acc: 0.9299, Val Loss: 0.2075, Val Acc: 0.6993\n",
            "Epoch [29/80], Train Loss: 0.1240, Train Acc: 0.9318, Val Loss: 0.2000, Val Acc: 0.7466\n",
            "Epoch [30/80], Train Loss: 0.1231, Train Acc: 0.9356, Val Loss: 0.1743, Val Acc: 0.8176\n",
            "Epoch [31/80], Train Loss: 0.1250, Train Acc: 0.9280, Val Loss: 0.2099, Val Acc: 0.7331\n",
            "Epoch [32/80], Train Loss: 0.1224, Train Acc: 0.9417, Val Loss: 0.1638, Val Acc: 0.7905\n",
            "Epoch [33/80], Train Loss: 0.1203, Train Acc: 0.9485, Val Loss: 0.1734, Val Acc: 0.7804\n",
            "Epoch [34/80], Train Loss: 0.1200, Train Acc: 0.9420, Val Loss: 0.1859, Val Acc: 0.7162\n",
            "Epoch [35/80], Train Loss: 0.1231, Train Acc: 0.9333, Val Loss: 0.1687, Val Acc: 0.8209\n",
            "Epoch [36/80], Train Loss: 0.1216, Train Acc: 0.9330, Val Loss: 0.1663, Val Acc: 0.8074\n",
            "Epoch [37/80], Train Loss: 0.1171, Train Acc: 0.9511, Val Loss: 0.1772, Val Acc: 0.7635\n",
            "Epoch [38/80], Train Loss: 0.1181, Train Acc: 0.9485, Val Loss: 0.1632, Val Acc: 0.8446\n",
            "Epoch [39/80], Train Loss: 0.1166, Train Acc: 0.9553, Val Loss: 0.2040, Val Acc: 0.7196\n",
            "Epoch [40/80], Train Loss: 0.1167, Train Acc: 0.9576, Val Loss: 0.1697, Val Acc: 0.8074\n",
            "Epoch [41/80], Train Loss: 0.1185, Train Acc: 0.9436, Val Loss: 0.1809, Val Acc: 0.7500\n",
            "Epoch [42/80], Train Loss: 0.1153, Train Acc: 0.9530, Val Loss: 0.1777, Val Acc: 0.7973\n",
            "Epoch [43/80], Train Loss: 0.1161, Train Acc: 0.9549, Val Loss: 0.1731, Val Acc: 0.7973\n",
            "Epoch [44/80], Train Loss: 0.1154, Train Acc: 0.9564, Val Loss: 0.1730, Val Acc: 0.7838\n",
            "Epoch [45/80], Train Loss: 0.1162, Train Acc: 0.9508, Val Loss: 0.1854, Val Acc: 0.7568\n",
            "Epoch [46/80], Train Loss: 0.1174, Train Acc: 0.9470, Val Loss: 0.1738, Val Acc: 0.7939\n",
            "Epoch [47/80], Train Loss: 0.1104, Train Acc: 0.9689, Val Loss: 0.1778, Val Acc: 0.7804\n",
            "Epoch [48/80], Train Loss: 0.1136, Train Acc: 0.9561, Val Loss: 0.1635, Val Acc: 0.8378\n",
            "Epoch [49/80], Train Loss: 0.1100, Train Acc: 0.9667, Val Loss: 0.1936, Val Acc: 0.7230\n",
            "Epoch [50/80], Train Loss: 0.1122, Train Acc: 0.9602, Val Loss: 0.1869, Val Acc: 0.8007\n",
            "Epoch [51/80], Train Loss: 0.1095, Train Acc: 0.9705, Val Loss: 0.1817, Val Acc: 0.7466\n",
            "Epoch [52/80], Train Loss: 0.1126, Train Acc: 0.9602, Val Loss: 0.1671, Val Acc: 0.8176\n",
            "Epoch [53/80], Train Loss: 0.1127, Train Acc: 0.9568, Val Loss: 0.1801, Val Acc: 0.7500\n",
            "Epoch [54/80], Train Loss: 0.1106, Train Acc: 0.9629, Val Loss: 0.1787, Val Acc: 0.7432\n",
            "Epoch [55/80], Train Loss: 0.1112, Train Acc: 0.9625, Val Loss: 0.1655, Val Acc: 0.8108\n",
            "Epoch [56/80], Train Loss: 0.1087, Train Acc: 0.9701, Val Loss: 0.1837, Val Acc: 0.7466\n",
            "Epoch [57/80], Train Loss: 0.1104, Train Acc: 0.9625, Val Loss: 0.1687, Val Acc: 0.8108\n",
            "Epoch [58/80], Train Loss: 0.1063, Train Acc: 0.9769, Val Loss: 0.1724, Val Acc: 0.8074\n",
            "Epoch [59/80], Train Loss: 0.1073, Train Acc: 0.9746, Val Loss: 0.1712, Val Acc: 0.7804\n",
            "Epoch [60/80], Train Loss: 0.1091, Train Acc: 0.9708, Val Loss: 0.1612, Val Acc: 0.8007\n",
            "Epoch [61/80], Train Loss: 0.1072, Train Acc: 0.9723, Val Loss: 0.1638, Val Acc: 0.8243\n",
            "Epoch [62/80], Train Loss: 0.1076, Train Acc: 0.9701, Val Loss: 0.1558, Val Acc: 0.8209\n",
            "Epoch [63/80], Train Loss: 0.1055, Train Acc: 0.9739, Val Loss: 0.1595, Val Acc: 0.8345\n",
            "Epoch [64/80], Train Loss: 0.1062, Train Acc: 0.9708, Val Loss: 0.1694, Val Acc: 0.8041\n",
            "Epoch [65/80], Train Loss: 0.1069, Train Acc: 0.9720, Val Loss: 0.1555, Val Acc: 0.8649\n",
            "Epoch [66/80], Train Loss: 0.1076, Train Acc: 0.9689, Val Loss: 0.1699, Val Acc: 0.8108\n",
            "Epoch [67/80], Train Loss: 0.1068, Train Acc: 0.9712, Val Loss: 0.1712, Val Acc: 0.7736\n",
            "Epoch [68/80], Train Loss: 0.1042, Train Acc: 0.9769, Val Loss: 0.1727, Val Acc: 0.7973\n",
            "Epoch [69/80], Train Loss: 0.1061, Train Acc: 0.9712, Val Loss: 0.1617, Val Acc: 0.7905\n",
            "Epoch [70/80], Train Loss: 0.1059, Train Acc: 0.9720, Val Loss: 0.1950, Val Acc: 0.7635\n",
            "Epoch [71/80], Train Loss: 0.1083, Train Acc: 0.9739, Val Loss: 0.1532, Val Acc: 0.8446\n",
            "Epoch [72/80], Train Loss: 0.1047, Train Acc: 0.9758, Val Loss: 0.1709, Val Acc: 0.8074\n",
            "Epoch [73/80], Train Loss: 0.1044, Train Acc: 0.9822, Val Loss: 0.1652, Val Acc: 0.8176\n",
            "Epoch [74/80], Train Loss: 0.1056, Train Acc: 0.9720, Val Loss: 0.1658, Val Acc: 0.8277\n",
            "Epoch [75/80], Train Loss: 0.1039, Train Acc: 0.9780, Val Loss: 0.1548, Val Acc: 0.8547\n",
            "Epoch [76/80], Train Loss: 0.1031, Train Acc: 0.9765, Val Loss: 0.1634, Val Acc: 0.8074\n",
            "Epoch [77/80], Train Loss: 0.1039, Train Acc: 0.9769, Val Loss: 0.1741, Val Acc: 0.7905\n",
            "Epoch [78/80], Train Loss: 0.1047, Train Acc: 0.9727, Val Loss: 0.1656, Val Acc: 0.8243\n",
            "Epoch [79/80], Train Loss: 0.1028, Train Acc: 0.9826, Val Loss: 0.1618, Val Acc: 0.8311\n",
            "Epoch [80/80], Train Loss: 0.1017, Train Acc: 0.9864, Val Loss: 0.1663, Val Acc: 0.8108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-06T07:21:09.118432Z",
          "iopub.status.busy": "2024-07-06T07:21:09.117902Z",
          "iopub.status.idle": "2024-07-06T09:16:30.231721Z",
          "shell.execute_reply": "2024-07-06T09:16:30.230348Z",
          "shell.execute_reply.started": "2024-07-06T07:21:09.118402Z"
        },
        "trusted": true,
        "id": "eiA-bTGgs1vV"
      },
      "outputs": [],
      "source": [
        "# weight_decays = [0.0,0.01,0.001]\n",
        "# label_smoothings = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
        "# lrs = [0.001]#, 0.0001]\n",
        "# channels = [16]\n",
        "# batches = [8]\n",
        "# #p_dropout = [0.1,0.3,0.5]\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=True)\n",
        "# #test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
        "\n",
        "# epochs = 80\n",
        "# test_iteration = 1\n",
        "# total_test = len(weight_decays)*len(label_smoothings)*len(lrs)*len(channels)*len(batches)#*len(p_dropout)\n",
        "# #Training with: weight_decay=0.0, label_smoothing=0.5, lr=0.001, channels=16, batch_size=8\n",
        "\n",
        "# for weight_decay in weight_decays:\n",
        "#     for label_smoothing in label_smoothings:\n",
        "#         for lr in lrs:\n",
        "#             for channel in channels:\n",
        "#                 for batch in batches:\n",
        "#                     model = ExtendedCNN(channels=channel, num_classes=num_classes,p_dropout=0)\n",
        "#                     model.to(device)\n",
        "\n",
        "#                     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "#                     print(f\"Training with: weight_decay={weight_decay}, label_smoothing={label_smoothing}, lr={lr}, channels={channel}, batch_size={batch},test={test_iteration}/{total_test}\")\n",
        "#                     test_iteration+=1\n",
        "#                     best_val_acc, best_params, best_epoch, found = train_loop(\n",
        "#                         model,\n",
        "#                         train_loader,\n",
        "#                         epochs,\n",
        "#                         optimizer,\n",
        "#                         val_loader,\n",
        "#                         verbose=True,\n",
        "#                         label_smoothing = label_smoothing,\n",
        "#                     )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d21484dba744589823686ea95ce27f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4733ff38e6f54f72a8dca7b6d1d6e1f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd9473ac34347508ec6f23534b463b2",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d21484dba744589823686ea95ce27f8",
            "value": 41
          }
        },
        "521788e7ab614c4bb1ec45d9853678db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d18c07c8179043df9447c93111a31dd0",
              "IPY_MODEL_4733ff38e6f54f72a8dca7b6d1d6e1f5",
              "IPY_MODEL_71a09f42d1f34c5faf7c012fbfe5fc1e"
            ],
            "layout": "IPY_MODEL_f7ded3abfb644ed3aaa967d1bc313eb6"
          }
        },
        "6a5d0a75de624cc5a80388acd0cafad1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a09f42d1f34c5faf7c012fbfe5fc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5d0a75de624cc5a80388acd0cafad1",
            "placeholder": "​",
            "style": "IPY_MODEL_84f66999ecb34dd284cbb5b1961e009c",
            "value": " 41/50 [07:47&lt;01:42, 11.43s/it]"
          }
        },
        "7b39c4ee962d4b019efe256eb5bc4b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd9473ac34347508ec6f23534b463b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f66999ecb34dd284cbb5b1961e009c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d18c07c8179043df9447c93111a31dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_febae8e0d3764652bbf57351fbfb5041",
            "placeholder": "​",
            "style": "IPY_MODEL_7b39c4ee962d4b019efe256eb5bc4b42",
            "value": " 82%"
          }
        },
        "f7ded3abfb644ed3aaa967d1bc313eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "febae8e0d3764652bbf57351fbfb5041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}