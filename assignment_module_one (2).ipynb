{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# **Product Recognition of Food Products**\n",
        "\n",
        "## Image Processing and Computer Vision - Assignment Module \\#1\n",
        "\n",
        "\n",
        "Contacts:\n",
        "\n",
        "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
        "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
        "- Alex Costanzino -> alex.costanzino@unibo.it\n",
        "- Francesco Ballerini -> francesco.ballerini4@unibo.it\n",
        "\n",
        "\n",
        "Computer vision-based object detection techniques can be applied in super market settings to build a system that can identify products on store shelves.\n",
        "An example of how this system could be used would be to assist visually impaired customers or automate common store management tasks like detecting low-stock or misplaced products, given an image of a shelf in a store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW42NlZsyTv0"
      },
      "source": [
        "## Task\n",
        "Develop a computer vision system that, given a reference image for each product, is able to identify such product from one picture of a store shelf.\n",
        "\n",
        "<figure>\n",
        "<a href=\"https://imgbb.com/\">\n",
        "  <center>\n",
        "  <img src=\"https://i.ibb.co/TwkMWnH/Screenshot-2024-04-04-at-14-54-51.png\" alt=\"Screenshot-2024-04-04-at-14-54-51\" border=\"0\" width=\"300\" />\n",
        "</a>\n",
        "</figure>\n",
        "\n",
        "For each type of product displayed in the\n",
        "shelf the system should report:\n",
        "1. Number of instances;\n",
        "1. Dimension of each instance (width and height in pixel of the bounding box that enclose them);\n",
        "1. Position in the image reference system of each instance (center of the bounding box that enclose them).\n",
        "\n",
        "#### Example of expected output\n",
        "```\n",
        "Product 0 - 2 instance found:\n",
        "  Instance 1 {position: (256, 328), width: 57px, height: 80px}\n",
        "  Instance 2 {position: (311, 328), width: 57px, height: 80px}\n",
        "Product 1 – 1 instance found:\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n",
        "### Track A - Single Instance Detection\n",
        "Develop an object detection system to identify single instance of products given one reference image for each item and a scene image.\n",
        "\n",
        "The system should be able to correctly identify all the product in the shelves\n",
        "image.\n",
        "\n",
        "### Track B - Multiple Instances Detection\n",
        "In addition to what achieved at step A, the system should also be able to detect multiple instances of the same product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fIbZJKq16ba"
      },
      "source": [
        "## Data\n",
        "Two folders of images are provided:\n",
        "* **Models**: contains one reference image for each product that the system should be able to identify.\n",
        "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios. The images contained in this folder are corrupted by noise.\n",
        "\n",
        "#### Track A - Single Instance Detection\n",
        "* **Models**: {ref1.png to ref14.png}.\n",
        "* **Scenes**: {scene1.png to scene5.png}.\n",
        "\n",
        "#### Track B - Multiple Instances Detection\n",
        "* **Models**: {ref15.png to ref27.png}.\n",
        "* **Scenes**: {scene6.png to scene12.png}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjP3GCdujYlw",
        "outputId": "0fff9ca7-6446-47eb-9587-958e78132faa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " Classroom\t 'Oğuz Kağan Koçak Gantt Chart.gsheet'\t\t\t'Task 3.1.1.gdoc'\n",
            " ColabNotebooks  'Oğuz Kağan Koçak Task 1.2.1.gsheet'\n",
            " dataset.zip\t 'PROJECT DOCUMENTATION AND ROADMAP FOR TASK 4.1.gdoc'\n",
            "Archive:  dataset.zip\n",
            "replace __MACOSX/._dataset? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "#imports drive library from the module of the google.colab\n",
        "# drive library represents some functions for working with Google Drive in a Google Colab Notebook\n",
        "drive.mount('/content/drive',force_remount=True)   # mounts the Google Drive to the Google Colab runtime virtual file system\n",
        "# That mounting creates a link between Google Drive and Google Colab environment\n",
        "# Therefore, we can access the files which are stored in the Google Drive directly from Google Colab environment\n",
        "\n",
        "!ls /content/drive/MyDrive/  # running that command will display a list of files and directories in the specified directory\n",
        "!cp -r /content/drive/MyDrive/dataset.zip ./\n",
        "# '/content/drive' represents the point of the mount that is a directory path in the Google Colab runtime\n",
        "# thus, we can access to the some data of the Google Drive under the '/content/drive' directory\n",
        "\n",
        "#'!cp -r' command copies the file 'dataset.zip' from the specified Google Drive location\n",
        "#to the current working directory(represented by './')\n",
        "# '-r' indicates that to copy the entire directory sturcture if 'dataset.zip' is a folder. Otherwise it is ignored\n",
        "\n",
        "!unzip dataset.zip\n",
        "#unzips the 'dataset.zip' file which was copied to the current working directory\n",
        "# this assumes the unzip tool is suitable on the Google Colab runtime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRBeGbKsEDe"
      },
      "source": [
        "## Evaluation criteria\n",
        "1. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n",
        "\n",
        "2. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n",
        "\n",
        "3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9O3Cj7cuXX3"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXmwWG3tBDLZ"
      },
      "outputs": [],
      "source": [
        "import cv2 # imports the OpenCV library, that is a computer vision library used for many tasks of image processing\n",
        "import numpy as np # imports the NumPy library, that is used for numerical computations\n",
        "import matplotlib.pyplot as plt # imports the pyplot module from the matplotlib library for using visualizations and plotting\n",
        "from google.colab.patches import cv2_imshow # that function is used to display the original image from the corresponding numpy array\n",
        "\n",
        "\n",
        "#last part, we copied the dataset.zip file to the current working directory(/content), then unzipped that file\n",
        "# therefore, we write the correct path to read the 'ref1.png' image\n",
        "image = cv2.imread('dataset/models/ref1.png') # reads an image file named 'fruit_image.jpg' from the current directory using OpenCV's imread() function\n",
        "# the image is loaded as a NumPy array\n",
        "# this image has BGR format in its numpy array\n",
        "\n",
        "print(image[360])\n",
        "# if the BGR or RGB values are very close to each other, the corresponding color converges to the white color\n",
        "# there are some historical reasons for the adoption of BGR in computer vision and image processing\n",
        "# Actually, RGB is more intutive for perception of human and suitable with in web development and graphic design\n",
        "\n",
        "\n",
        "\n",
        "print(\"=======================================================\")\n",
        "resized_image = cv2.resize(image, (300,300)) # resizes the original image to a new size of 300x300 pixels using OpenCV's resize() function\n",
        "# the resized image is stored in the 'resized_image' variable\n",
        "# there are more than 1 interpolation methods for resizing function in OpenCV library\n",
        "# the method is 'cv2.INTER_LINEAR', which corresponds to bilinear interpolation.\n",
        "# that interpolation computes the new pixel value based on a weighted average of the 4 nearest pixels in the input image\n",
        "# that method is efficient for resizing images\n",
        "\n",
        "plt.imshow(cv2. cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "# displays image using Matplotlib's 'imshow()' function\n",
        "# the image's color channels are converted from BGR(default format used by OpenCV) to RGB with cv2.cvtColor()\n",
        "# Matplotlib expects images in RGB format\n",
        "\n",
        "\n",
        "\n",
        "plt.axis('off') # turns off the axis in the plot, removing the axis labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "# normalizes the pixel values of the resized image to the range [0,1]\n",
        "# the image is translated to type float32 of NumPy array by astype() function\n",
        "# each pixel value is divided by 255.0 to scale them down to the [0,1] range\n",
        "\n",
        "print(\"Original image: \")\n",
        "cv2_imshow(image) # displaying the original image from the corresponding numpy array\n",
        "\n",
        "print('Resized and normalized image: ')\n",
        "#cv2_imshow(normalized_image)\n",
        "\n",
        "# normalized_image is a BGR format with numpy arrays, therfore, first, we should convert it to the RGB format, then, corresponding image format to display\n",
        "plt.imshow(cv2.cvtColor(normalized_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# plt.imshow() function is to display the image\n",
        "# cv2.cvtColor() function convert the BGR format image to RGB format image because RGB format is necessary for the plt.imshow() function\n",
        "\n",
        "cv2.waitKey(0) # wait indefinetly until a key is pressed\n",
        "cv2.destroyAllWindows() #close all windows of OpenCV when the program has finished executing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgVKopehwHZH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ImageDataGenerator library is for image augmentation\n",
        "# cv2 library is for reading and preprocessing images\n",
        "# numpy library is for numerical operations\n",
        "# matplotlib.pyplot library is for plotting images\n",
        "\n",
        "\n",
        "generateData= ImageDataGenerator( #creating ImageDataGenerator object named 'generateData' various augmentation parameters\n",
        "\n",
        "    rotation_range = 15, # randomly rotation of images by up to 15 degrees\n",
        "    brightness_range = (0.7,1.2), # randomly set brightness  between 0.7 and 1.2\n",
        "    height_shift_range = 0.15, # randomly shift images vertically by up to %15 of the image height\n",
        "    width_shift_range = 0.15, # randomly shift images horizontally by up to %15 of the image width\n",
        "    horizontal_flip = True # randomly flip images horizontally\n",
        "# all of these parameters decide on if their values of rotation, brightness, height shift, width shift ranges change for the each images in the dataset\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q-KoXINEZ_d"
      },
      "outputs": [],
      "source": [
        "image2 = cv2.imread('dataset/models/ref1.png')\n",
        " # this imread function convert the image to corresponding numpy array according to BGR order\n",
        "\n",
        "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
        " # image2 numpy array with BGR format is converted to the RGB format by using cvtColor function from the cv2 library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKBSa-Qo1tmS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "image2 = np.expand_dims(image2, axis=0)\n",
        "\n",
        "# let's assume that, we have RGB image with (height, width, channels) = (200, 300, 3)\n",
        "# channels means that, different components of color information in the each pixel of the image (Red, Green, Blue)\n",
        "# np.expand_dims(image2, axis=0) means that, resulting array will have dimensions which are (sizeOfBatch, height, width, channels) = (1, 200, 300, 3)\n",
        "# where the sizeOfBatch = 1, indicates that, there is only 1 image in the batch\n",
        "# flow() function of ImageDataGenerator expects input in that format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bamp5O8F8I9"
      },
      "outputs": [],
      "source": [
        "imagesAugmented = generateData.flow(image2)\n",
        "# generating augmented images using the function of flow which is the function of the generateData object\n",
        "# steps parameter means that, number of the total batches of the images before stopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFCvUBTFBBsw"
      },
      "outputs": [],
      "source": [
        "figure, gridOfSubplots = plt.subplots(1,4, figsize=(20, 5))\n",
        "\n",
        "# subplots function creates a figure and grid of subplots with 4 columns and 1 row\n",
        "# figsize=(20,5) parameter determines the each figure's size, where the height is 5, width is 20 inches\n",
        "\n",
        "type(imagesAugmented.next()[0])\n",
        "\n",
        "for i in range(4):  # iterating over 4 times to display images which are augmented\n",
        "# 4 is the batch size (number of images to display)\n",
        "\n",
        "\n",
        "  each_imageAugmented = imagesAugmented.next()[0].astype(np.uint8)\n",
        "\n",
        "  # imagesAugmented.next() generates the next augmented images's batch\n",
        "  # in each call of next function, will return the next augmented image (batch)\n",
        "  # [0] is for selecting the first element of the augmented images(batch)\n",
        "  # at the same time, each batch contains both images and corresponding labels\n",
        "  # astype(np.uint8) converts the image's pixel values to unsigned 8 bit integers because that is the standard format for displaying image\n",
        "  # if the image was loaded using cv2.imread() function, the pixel values are represented as unsigned 8 bit integers\n",
        "\n",
        "  # cv2.imread() function reads images as BGR(blue, green, red) format and stores these pixel values as unsigned 8 bit integers\n",
        "\n",
        "  gridOfSubplots[i].imshow(each_imageAugmented)\n",
        "  # displays the batch(augmented image) on the th of  \"i\" subplot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEiNN9D86weM"
      },
      "outputs": [],
      "source": [
        "!pwd # printing the current working directory (content) in Google Colab\n",
        "\n",
        "!ls # to list the contents of the directory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXl-6v0hi7hf"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/ColabNotebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/assignment_module_one.ipynb /content/IPCVAssignment1\n",
        "# to copy the assignment_module_one.ipynb file from the corresponding path to the /content/IPCVAssignment1 directory"
      ],
      "metadata": {
        "id": "zdVKsd27g2_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/IPCVAssignment1/ #to list the contents"
      ],
      "metadata": {
        "id": "PWWjQWhnhj2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wjaZ3zLpWxx"
      },
      "outputs": [],
      "source": [
        "%cd /content/IPCVAssignment1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless==4.5.5.64\n"
      ],
      "metadata": {
        "id": "4vpM7r3QkMzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "by installing that, we are setting up the necessary image processing capabilities in our Google Colab environment to work on that project\n",
        " headless version is smaller in size and does not have any GUI dependencies (Google Colab do not require a graphical user interface)\n",
        " GUI is a user interface type which allows users to interact with electronic devices by visual indicators and graphical icons instead of command-line prompts or text-based interfaces"
      ],
      "metadata": {
        "id": "Km5KxWFzvUNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "rk3HKiucwAxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ! indicates that to allowing to run shell commands directly from the notebook cells\n",
        "- !pip install command is to install Python packages from the Python Package Index (PYPI)\n",
        "- to install Tensorflow library which is for ML- powered applications\n",
        "- to install Matplotlib which is a plotting library for the Python programming language"
      ],
      "metadata": {
        "id": "XvpV3S-YwZvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "JhQ4LFQEzFts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- importing libraries from the installed packages to use in this project\n",
        "- Keras is an API for minimizing the number of user actions required for common cases, which is also ensures clear feedback upon user error."
      ],
      "metadata": {
        "id": "wHDOg4IqzNT3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxkvTuCBjq0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imagePreprocess(pathOfImage):\n",
        "  image = cv2.imread(pathOfImage, cv2.IMREAD_COLOR)\n",
        "  # to read the image from the specified file path using the 'cv2.IMREAD_COLOR' function\n",
        "  # to clarify each pixel's BGR(default color format used by OpenCV) values\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  # to convert the image from BGR to RGB color format which is used by most other libraries like tensorflow and matplotlib\n",
        "  # thus, the image colors are displayed with correct values of the corresponding image values\n",
        "  imageResized = cv2.resize(image, (250,250))  # to resize the image as 250x250 pixels\n",
        "  # this function is significant for batch processing in machine learning models\n",
        "  imageNormalized = imageResized.astype(np.float32) / 255.0\n",
        "  # this code line provides basic representation of the corresponding color values for each pixel\n",
        "  return imageNormalized\n",
        "\n",
        "imagePreprocessing = imagePreprocess('dataset/models/ref1.png')\n",
        "# to preprocess for a picture using with imagePreprocess function\n",
        "plt.imshow(imagePreprocessing) # to show the colorful image of the  corresponding normalized pixel values\n",
        "plt.axis('on')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zGCRJ41Ez5Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "# VGG16(develooped by Visual Geometry Group,\n",
        "# 16 refers to the number of layers with trainable parameters such as convolutional and fully connected layers)\n",
        "# which is pre-trained Convolutional Neural Network arhitecture from the keras module\n",
        "# that architecture consists of 16 layers which include convolutional layers, fully connected layers and pooling layers\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "# Model is to create a new model based on the pre-trained VGG16 model\n",
        "\n",
        "modelDefault =VGG16(weights = 'imagenet', include_top=False, input_shape=(250,250,3))\n",
        "#  weights = 'imagenet' means that, the model should load pre-trained weights\n",
        "# imagesnet refers to the large dataset used to train this model\n",
        "# contains millions of images related to the many categories\n",
        "# the model can leverage the knowledge it gained from the imagenet to be beneficial for various image proessing tasks using these pre-trained weights\n",
        "# even dataset is small\n",
        "\n",
        "# include_top=False, top means that, the final fully connected layers of the model that are used for classification\n",
        "# above code exclude these layers, this is not beneficial for VGG16's original classification aim but is beneficial for feature extractor ,\n",
        "# this code is beneficial for adding own custom layers on top of the VGG16 model to the specific task\n",
        "# input_shape=(250,250,3) represents that, 250 pixels for height, 250 pixels for width and 3 pixels for Red, Green and Blue color channels\n",
        "\n",
        "featurExtractor = Model(inputs=modelDefault.input, outputs=modelDefault.get_layer('block5_conv3').output)\n",
        "# Model() function allows to create a new model from the Keras library\n",
        "# inputs = modelDefault.input means that, the input of this new model is the same with the input of the original VGG16 model which is (250,250,3)\n",
        "# outputs = modelDefault.get_layer('block5_conv3').output means that, the output of the this new model is the output of the layer named 'block5_conv3' in the VGG16 model\n",
        "# this layer is a convolutional layer towards the end of the network, capturing high-level features of the image\n",
        "# layers which are closer to the input capture low-level features(like textures and edges)\n",
        "# layers which are deeper layers (like 'block5_conv3) capture high-level features(lke objects and shapes)\n",
        "# 'block5_conv3' has more information. Therefore, it is useful for image recognition and object detection tasks\n",
        "# output means that, output of the 'block5_conv3' layer feature map\n",
        "\n",
        "def featurExtracting(imageArr): # imageArr is preprocessed(transformed to RGB color ) and normalized ([0-1] range) image\n",
        "  imageArr = np.expand_dims(imageArr,axis=0) # to transform the shape to (1,250,250,3),  1 indicates that, there is 1 image in this batch\n",
        "  return featurExtractor.predict(imageArr) # predict method of the model takes the batch of the images, processes them to extract feature maps from the input\n",
        "\n",
        "extractedFeatures = featurExtracting(imagePreprocessing)\n",
        "print(extractedFeatures.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nk_vpqITnlvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- FEATURE EXTRACTION BY CNN\n",
        "- (1, 15, 15, 512)  means that,\n",
        "- 1  is the batch size, because we are processing 1 image at a time\n",
        "- 15,15 are spatial dimensions of the extracted feature map\n",
        "- we resized the input image to the 250x250 pixels, then, when we reach to the 'block5_conv3' convolutional layer, the spatial dimensions become 15x15 pixels\n",
        "- 512 is the number of VGG16 model filters(feature map channels)\n",
        "\n"
      ],
      "metadata": {
        "id": "xd9nVbpIzN36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matchingOfObjectInScene(featuresScene,featuresObject):\n",
        "  # scene image is a larger image which contains the entire scene where we are searching for the object(template)\n",
        "  # For instance, it(scene image) could be a photograph of the supermarket shelf\n",
        "\n",
        "  # object image is a smaller image which contains a specific object we are searching for within the scene\n",
        "  # to illustrate, it could be a single product image we are searching on the supermarket shelf\n",
        "\n",
        "  # featuresScene specifies of the larger image(scene image which has multiple instances)\n",
        "  # featuresObject specifies of the smaller image(instance image), we try to find this instance from the scene\n",
        "\n",
        "  # featuresScene and featuresObject are 2D array matrices\n",
        "  # with feature extraction, featuresScene and featuresObject are feature maps generated by a Convolutional Neural Network from the scene and template images\n",
        "  # without feature extraction, featuresScene and featuresObject are grayscale pixel value(0 to 255) matrices of the template and scene images\n",
        "\n",
        "  similarityValuesOfEachRegion = cv2.matchTemplate(featuresScene, featuresObject, cv2.TM_CCOEFF_NORMED)\n",
        "  # cv2.matchTemplate used for instance matching, this function slides template image over the scene image\n",
        "  # and compares the template and the scene image's region under that template\n",
        "\n",
        "  # TM_CCOEFF_NORMED stands for normalized cross-correlation method by OpenCV\n",
        "  # it compares the template and image regions in the scene based on their coefficients of corelation\n",
        "  # that function is sensitive to changes in brightness among of the templates however, it can be more robust into the contrast variations\n",
        "  # cv2.TM_CCOEFF_NORMED function represents the matching method\n",
        "\n",
        "  # the similarityValuesOfEachRegion contains the similarity values between the template and the different regions of the scene image\n",
        "  # the higher values indicating that better matches\n",
        "  # the dimensions of similarityValuesOfEachRegion variable depend on the scene and template images's sizes\n",
        "\n",
        "  minValue, maxValue, minLoc, maxLoc = cv2.minMaxLoc(similarityValuesOfEachRegion)\n",
        "  # minMaxLoc function finds the minimum and maximum matching values and their locations on the scene image\n",
        "\n",
        "  return maxValue, maxLoc\n",
        "\n",
        "  sceneImage = imagePreprocess('dataset/scenes/scene1.png')\n",
        "  # with the imagePreprocess function, scene1.png is resized, transformed to RGB color and normalized ([0-1]) range\n",
        "  sceneFeatures = featurExtracting(sceneImage)\n",
        "  # with the featurExtracting function, 512 times 15x15 spatial dimensions(extracted feature maps) are generated from the sceneImage variable\n",
        "  objectFeatures = featurExtracting(imagePreprocessing)\n",
        "  # with the featurExtracting function, 512 times 15x15 spatial dimensions (extracted feature maps) are generated from the preprocessed ref1.png image\n",
        "\n",
        "  bestvalueOfMatching, itslocation = matchingOfObjectInScene(sceneFeatures, objectFeatures)\n",
        "\n",
        "  print(f\"The best value of the matching is: {bestvalueOfMatching}, and its location is: {itslocation}\" )\n",
        "\n"
      ],
      "metadata": {
        "id": "NL_8XaWb1pHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizeOfReferenceObject = imagePreprocessing.shape[0:2]\n",
        "# imagePreprocessing variable is the resized and normalized version of the reference image\n",
        "# imagePreprocessing.shape function outputs a tuple form like (number of horizontal pixels, number of vertical pixels, 3 values for 3 RGB channels)\n",
        "# therefore, we used the slicing operation([0:2]) to ignore the 3 RGB color values\n",
        "\n"
      ],
      "metadata": {
        "id": "cZukSiCRnY3F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b3fca876-3cc5-4949-8e3a-9b36bed578cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'imagePreprocessing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1596eabac7f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msizeOfReferenceObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimagePreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# imagePreprocessing variable is the resized and normalized version of the reference image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# imagePreprocessing.shape function outputs a tuple form like (number of horizontal pixels, number of vertical pixels, 3 values for 3 RGB channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# therefore, we used the slicing operation([0:2]) to ignore the 3 RGB color values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'imagePreprocessing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CenterLocOfDetectedProducts(sizeOfReferenceObject, itslocation):\n",
        "  # this function calculate the center position of a bounding box of a detected object in a scene image\n",
        "  # this bounding box provides information about its size and position\n",
        "\n",
        "  # sizeOfReferenceObject specifies (width, height) (size of the reference template image) which is used for object detection in the scene image\n",
        "\n",
        "  # itslocation parameter specifies the (x,y) (top-left distance from the edges)\n",
        "  # (x,y) location represents where the detected object starts in the scene image during the processing of template matching\n",
        "\n",
        "  width = sizeOfReferenceObject[0] # to assign width value of the reference object to the width variable\n",
        "  height = sizeOfReferenceObject[1]  # to assign height value of the reference object to the height variable\n",
        "  x,y = itslocation # x and y values are assigned to the corresponding position values\n",
        "\n",
        "  halfOfX = width // 2 + x  # to find the center position of the x variable\n",
        "  halfOfY = height // 2 + y  # to find the center position of the y variable\n",
        "\n",
        "  return (halfOfX, halfOfY)  # to return the center location of the detected product as tuple\n"
      ],
      "metadata": {
        "id": "_HZwpgnxJ4A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Center position of the bounding box is: \", {CenterLocOfDetectedProducts(sizeOfReferenceObject, itslocation)})\n",
        "# to print the center position of the detected reference image from the scene image\n"
      ],
      "metadata": {
        "id": "CaRz-uJzlzAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.image import non_max_suppression\n",
        "\n",
        "def matchingMultipleInstancesInScene(threshold = 0.65, featuresObject, featuresScene):\n",
        "  # this function provides detecting multiple instances of a template product image within a self image\n",
        "  # to find a potential matches within the self image, we are using template matching\n",
        "\n",
        "  # threshold = 0.65 represents that,\n",
        "  # if the matching value of the specified position within the shelf image while detecting the reference product is equal or greater than 0.65,\n",
        "  # it will be considered as potential matches\n",
        "\n",
        "  #  featuresObject are the features of the reference product object which is extracted from the neural network\n",
        "  # that is template image to be found in the scene image\n",
        "\n",
        "  # featuresScene are the features of the shelf image which is extracted from a neural network\n",
        "  # that is image in which to find for the template image\n",
        "\n",
        "  locOfdetectedTemplate = [] # to store the detected instances from the shelf image\n",
        "  similarityValueDetectedInstance = [] # to store the similarity values of the detected instances\n",
        "\n",
        "\n",
        "  similarityValueMatrix = cv2.matchTemplate(featuresScene, featuresObject, cv2.TM_CCOEFF_NORMED)\n",
        "  # cv2.matchTemplate function used for matching\n",
        "  # that uses the normalized cross-correlation coefficient\n",
        "\n",
        "  # similarityValueMatrix variable represents similarity rates of the each region of the shelf image\n",
        "  # into the this similarityValueMatrix, the similarity values are stored with corresponding starting point of the detected template image in the shelf image\n",
        "\n",
        "  matchedPositions = np.where(threshold <= similarityValueMatrix)\n",
        "  # np.where function used to return the location of the specified region of the shelf image where similarity rate is calculated\n",
        "  # to find the positions within the shelf image where the similarity values are equal or greater than the threshold value\n",
        "\n",
        "  reversedMatchedPositions = matchedPositions[ : : -1]\n",
        "  # to reverse the this tuple's order because we need to loop through (x,y) ((index of column, index of row)) not as (index of row, index of column)\n",
        "\n",
        "  for eachPosition in zip(*reversedMatchedPositions):\n",
        "    # * means that, to unpack this tuple arrays into the 2 seperate arrays like [indexes of columns] and [indexes of rows]\n",
        "    # zip function means that, pairs up the\n",
        "    # that is a best practice to loop through each detected instances easily\n",
        "\n",
        "    # eachPosition variable is updated as (index of column, index of row) or (horizontal distance, vertical distance)\n",
        "    locOfdetectedTemplate.append(eachPosition[0], eachPosition[1], eachPosition[0]+width, eachPosition[1]+height)\n",
        "    # to store the starting and ending points(top-left and bottom-right points) of the detected template from the shelf image\n",
        "    positionCenter = CenterLocOfDetectedProducts(sizeOfReferenceObject, eachPosition)\n",
        "\n",
        "    similarityValueDetectedInstance.append(similarityValueMatrix[eachPosition[0], eachPosition[1]])\n",
        "    # similarityValueMatrix[eachPosition[0], eachPosition[1]] indicates that similarity value of the corresponding detected template image from the shelf image,\n",
        "    # its starting point is eachPosition[0], eachPosition[1]\n",
        "\n",
        "  locOfdetectedTemplate = np.array(locOfdetectedTemplate)\n",
        "  similarityValueDetectedInstance = np.array(similarityValueDetectedInstance)\n",
        "\n",
        "  # to convert to a numpy array format\n",
        "\n",
        "\n",
        "  selectedDetectedImages = non_max_supression(locOfdetectedTemplate, similarityValueDetectedInstance, max_output_size = 20, iou_threshold = 0.55)\n",
        "  # non_max_supression function examines all of the detected template images and their similarity values to filter out the less significant ones\n",
        "  # this means that, only keeps the template images which have highest similarity values while removing overlapped detected objects\n",
        "\n",
        "  #  max_output_size = 20 means that, to only keep the 20 detected template images which have highest similarity value in the shelf image\n",
        "\n",
        "  #  iou_threshold = 0.4 means that, (intersection over union) to decide the threshold parameter for when to suppress a detected object\n",
        "  # thus, between the overlapped detected images, if the intersection over union rate of these detected objects is greater than 0.55, the detected image with lower similarity value is removed\n",
        "\n",
        "  # selectedDetectedImages is a tensor(multi-dimensional arrays(single numbers are included))\n",
        "  # selectedDetectedImages keeps the data of the starting point of the selected detected images\n",
        "\n",
        "  selectedDetectedImages = np.array(selectedDetectedImages) # convert tensor to numpy array format\n",
        "  selectedObjects = locOfdetectedTemplate[selectedDetectedImages] # to select corresponding object from its location(selectedDetectedImages)\n",
        "\n",
        "  return selectedObjects\n",
        "\n",
        "\n",
        "finalDetectedObjects = matchingMultipleInstancesInScene((threshold = 0.65, featuresObject, featuresScene))\n",
        "\n",
        "print(f\"The detected objects are: \"{finalDetectedObjects}) # to print the final detected objects from the scene image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yoWs_TRg_Z5k",
        "outputId": "920f675e-d766-4882-d4bf-186d856315d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "non-default argument follows default argument (<ipython-input-1-1f4ca8c5096f>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1f4ca8c5096f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def matchingMultipleInstancesInScene(threshold = 0.65, featuresObject, featuresScene):\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
