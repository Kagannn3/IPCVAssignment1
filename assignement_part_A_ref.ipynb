{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOewfPneB2Ow+tmSZAHInqt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1l3ens7kZhH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716559555840,"user_tz":-120,"elapsed":21920,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}},"outputId":"1bfe44b2-500e-4122-f936-a1207f0b3c8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Archive:  dataset.zip\n","   creating: dataset/\n","  inflating: __MACOSX/._dataset      \n","   creating: dataset/scenes/\n","  inflating: __MACOSX/dataset/._scenes  \n","  inflating: dataset/.DS_Store       \n","  inflating: __MACOSX/dataset/._.DS_Store  \n","   creating: dataset/models/\n","  inflating: __MACOSX/dataset/._models  \n","  inflating: dataset/scenes/scene12.png  \n","  inflating: __MACOSX/dataset/scenes/._scene12.png  \n","  inflating: dataset/scenes/scene10.png  \n","  inflating: __MACOSX/dataset/scenes/._scene10.png  \n","  inflating: dataset/scenes/scene11.png  \n","  inflating: __MACOSX/dataset/scenes/._scene11.png  \n","  inflating: dataset/scenes/scene5.png  \n","  inflating: __MACOSX/dataset/scenes/._scene5.png  \n","  inflating: dataset/scenes/scene4.png  \n","  inflating: __MACOSX/dataset/scenes/._scene4.png  \n","  inflating: dataset/scenes/scene6.png  \n","  inflating: __MACOSX/dataset/scenes/._scene6.png  \n","  inflating: dataset/scenes/scene7.png  \n","  inflating: __MACOSX/dataset/scenes/._scene7.png  \n","  inflating: dataset/scenes/scene3.png  \n","  inflating: __MACOSX/dataset/scenes/._scene3.png  \n","  inflating: dataset/scenes/scene2.png  \n","  inflating: __MACOSX/dataset/scenes/._scene2.png  \n","  inflating: dataset/scenes/scene1.png  \n","  inflating: __MACOSX/dataset/scenes/._scene1.png  \n","  inflating: dataset/scenes/scene9.png  \n","  inflating: __MACOSX/dataset/scenes/._scene9.png  \n","  inflating: dataset/scenes/scene8.png  \n","  inflating: __MACOSX/dataset/scenes/._scene8.png  \n","  inflating: dataset/models/ref8.png  \n","  inflating: __MACOSX/dataset/models/._ref8.png  \n","  inflating: dataset/models/ref9.png  \n","  inflating: __MACOSX/dataset/models/._ref9.png  \n","  inflating: dataset/models/ref12.png  \n","  inflating: __MACOSX/dataset/models/._ref12.png  \n","  inflating: dataset/models/ref13.png  \n","  inflating: __MACOSX/dataset/models/._ref13.png  \n","  inflating: dataset/models/ref11.png  \n","  inflating: __MACOSX/dataset/models/._ref11.png  \n","  inflating: dataset/models/ref10.png  \n","  inflating: __MACOSX/dataset/models/._ref10.png  \n","  inflating: dataset/models/ref14.png  \n","  inflating: __MACOSX/dataset/models/._ref14.png  \n","  inflating: dataset/models/ref15.png  \n","  inflating: __MACOSX/dataset/models/._ref15.png  \n","  inflating: dataset/models/ref17.png  \n","  inflating: __MACOSX/dataset/models/._ref17.png  \n","  inflating: dataset/models/ref16.png  \n","  inflating: __MACOSX/dataset/models/._ref16.png  \n","  inflating: dataset/models/ref27.png  \n","  inflating: __MACOSX/dataset/models/._ref27.png  \n","  inflating: dataset/models/ref26.png  \n","  inflating: __MACOSX/dataset/models/._ref26.png  \n","  inflating: dataset/models/ref18.png  \n","  inflating: __MACOSX/dataset/models/._ref18.png  \n","  inflating: dataset/models/ref24.png  \n","  inflating: __MACOSX/dataset/models/._ref24.png  \n","  inflating: dataset/models/ref25.png  \n","  inflating: __MACOSX/dataset/models/._ref25.png  \n","  inflating: dataset/models/ref19.png  \n","  inflating: __MACOSX/dataset/models/._ref19.png  \n","  inflating: dataset/models/ref21.png  \n","  inflating: __MACOSX/dataset/models/._ref21.png  \n","  inflating: dataset/models/ref20.png  \n","  inflating: __MACOSX/dataset/models/._ref20.png  \n","  inflating: dataset/models/ref22.png  \n","  inflating: __MACOSX/dataset/models/._ref22.png  \n","  inflating: dataset/models/ref23.png  \n","  inflating: __MACOSX/dataset/models/._ref23.png  \n","  inflating: dataset/models/ref7.png  \n","  inflating: __MACOSX/dataset/models/._ref7.png  \n","  inflating: dataset/models/ref6.png  \n","  inflating: __MACOSX/dataset/models/._ref6.png  \n","  inflating: dataset/models/ref4.png  \n","  inflating: __MACOSX/dataset/models/._ref4.png  \n","  inflating: dataset/models/ref5.png  \n","  inflating: __MACOSX/dataset/models/._ref5.png  \n","  inflating: dataset/models/ref1.png  \n","  inflating: __MACOSX/dataset/models/._ref1.png  \n","  inflating: dataset/models/ref2.png  \n","  inflating: __MACOSX/dataset/models/._ref2.png  \n","  inflating: dataset/models/ref3.png  \n","  inflating: __MACOSX/dataset/models/._ref3.png  \n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n","!unzip dataset.zip"]},{"cell_type":"code","source":[],"metadata":{"id":"j-O3bi3MMBrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"altDGT0DZtnc","executionInfo":{"status":"ok","timestamp":1716559556797,"user_tz":-120,"elapsed":959,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def load_and_split_images(folder, split_point, color_mode=cv2.IMREAD_COLOR):\n","    images_first_part = []\n","    images_second_part = []\n","\n","    # Fonction pour extraire le numéro du nom du fichier\n","    def extract_number(filename):\n","        return int(''.join(filter(str.isdigit, filename)))\n","\n","    # Tri des fichiers par ordre numérique en extrayant le numéro du nom de fichier\n","    sorted_filenames = sorted(os.listdir(folder), key=extract_number)\n","\n","    for filename in sorted_filenames:\n","        img_path = os.path.join(folder, filename)\n","        if img_path.endswith(\".png\"):\n","            img = cv2.imread(img_path, color_mode)\n","            if img is not None:\n","                number = extract_number(filename)\n","                if number <= split_point:\n","                    images_first_part.append(img)\n","                else:\n","                    images_second_part.append(img)\n","            else:\n","                print(f\"Failed to load image at {img_path}\")\n","    return images_first_part, images_second_part\n"],"metadata":{"id":"UUgSgbX85hrD","executionInfo":{"status":"ok","timestamp":1716559558678,"user_tz":-120,"elapsed":250,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def load_and_split_images(folder, split_point, color_mode=cv2.IMREAD_COLOR):\n","    images_first_part = []\n","    images_second_part = []\n","    for filename in sorted(os.listdir(folder)):\n","        img_path = os.path.join(folder, filename)\n","        if img_path.endswith(\".png\"):\n","            img = cv2.imread(img_path, color_mode)\n","            if img is not None:\n","                number = int(filename.split('.')[0].replace('ref', '').replace('scene', ''))\n","                if number <= split_point:\n","                    images_first_part.append(img)\n","                else:\n","                    images_second_part.append(img)\n","            else:\n","                print(f\"Failed to load image at {img_path}\")\n","    return images_first_part, images_second_part"],"metadata":{"id":"xlRCSDUiZv4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_median_filter(images, kernel_size=5):\n","    filtered_images = []\n","    for img in images:\n","        filtered_img = cv2.medianBlur(img, kernel_size)\n","        filtered_images.append(cv2.cvtColor(filtered_img, cv2.COLOR_BGR2GRAY))  # Convert to grayscale\n","    return filtered_images"],"metadata":{"id":"4zUQ1MJVZyGh","executionInfo":{"status":"ok","timestamp":1716559564836,"user_tz":-120,"elapsed":260,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def angle_between_vectors(v1, v2):\n","    \"\"\"Calculate the angle in degrees between vectors 'v1' and 'v2'\"\"\"\n","    unit_v1 = v1 / np.linalg.norm(v1)\n","    unit_v2 = v2 / np.linalg.norm(v2)\n","    dot_product = np.clip(np.dot(unit_v1, unit_v2), -1.0, 1.0)\n","    return np.degrees(np.arccos(dot_product))"],"metadata":{"id":"YFYJBcjyZz7W","executionInfo":{"status":"ok","timestamp":1716559566495,"user_tz":-120,"elapsed":248,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def detect_and_mask_products(scene_image, model_images):\n","    sift = cv2.SIFT_create()\n","    index_params = dict(algorithm=1, trees=5)\n","    search_params = dict(checks=50)\n","    flann = cv2.FlannBasedMatcher(index_params, search_params)\n","\n","    detections = []\n","    masked_image = scene_image.copy()\n","    scene_height, scene_width = scene_image.shape[:2]\n","\n","    while True:\n","        scene_keypoints, scene_descriptors = sift.detectAndCompute(masked_image, None)\n","        max_matches = 0\n","        best_match = None\n","        best_model_keypoints = None\n","        best_homography = None\n","\n","        for model_idx, model in enumerate(model_images):\n","            model_keypoints, model_descriptors = sift.detectAndCompute(model, None)\n","            matches = flann.knnMatch(model_descriptors, scene_descriptors, k=2)\n","            good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n","\n","            if len(good_matches) > max_matches:\n","                max_matches = len(good_matches)\n","                best_match = model_idx\n","                best_model_keypoints = model_keypoints\n","\n","                if len(good_matches) > 10:\n","                    src_pts = np.float32([model_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","                    dst_pts = np.float32([scene_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","                    homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","                    best_homography = homography\n","\n","        if max_matches < 25:\n","            break  # Stop if new matches are significantly lower than previous or below the fixed threshold\n","\n","        if best_homography is not None:\n","            # Calculate coordinates of model in scene and mask it\n","            h, w = model_images[best_match].shape[:2]\n","\n","            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n","            dst = cv2.perspectiveTransform(pts, best_homography)\n","\n","\n","            # Calculate angles between consecutive corners\n","            angles = []\n","            num_corners = len(dst)\n","            for i in range(num_corners):\n","                v1 = dst[i][0] - dst[i - 1][0]\n","                v2 = dst[(i + 1) % num_corners][0] - dst[i][0]\n","                angle = angle_between_vectors(v1, v2)\n","                angles.append(angle)\n","\n","            print(angles)\n","            print(max_matches)\n","\n","            if any(60 > angle or angle > 120 for angle in angles):\n","              break\n","            # Skip detection if angles are not close to 90 degrees\n","            if not all(85 <= angle <= 95 for angle in angles):\n","                # Mask the detected area for processing but don't add to detections\n","                cv2.fillConvexPoly(masked_image, dst.astype(int), 0)\n","                continue\n","\n","            position = tuple(np.int32(dst[0, 0]))\n","            width = int(np.linalg.norm(dst[0][0] - dst[1][0]))\n","            height = int(np.linalg.norm(dst[0][0] - dst[3][0]))\n","\n","            detection_details = (best_match, max_matches, position, width, height, dst)\n","            detections.append(detection_details)\n","\n","            # Mask the detected area\n","            cv2.fillConvexPoly(masked_image, dst.astype(int), 0)\n","\n","    return masked_image, detections"],"metadata":{"id":"ggg_ZeWJZ1iE","executionInfo":{"status":"ok","timestamp":1716560018386,"user_tz":-120,"elapsed":397,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def draw_detections_on_image(image, detections, model_images):\n","    for detection in detections:\n","        best_match, max_matches, position, width, height, dst = detection\n","        color = (0, 255, 0)  # Green color for the rectangle\n","        cv2.polylines(image, [np.int32(dst)], True, color, 3)\n","        label = f'Product {best_match+1}'\n","        cv2.putText(image, label, position, cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n","    return image"],"metadata":{"id":"ImoFAGBEZlHF","executionInfo":{"status":"ok","timestamp":1716560137891,"user_tz":-120,"elapsed":229,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Setup paths\n","scenes_folder = '/content/dataset/scenes'\n","models_folder = '/content/dataset/models'\n","\n","# Load and separate images\n","model_images_first_part, _ = load_and_split_images(models_folder, 14)\n","scene_images_first_part, _ = load_and_split_images(scenes_folder, 5, cv2.IMREAD_COLOR)\n","\n","# Apply median filter and convert to grayscale\n","filtered_scene_images = apply_median_filter(scene_images_first_part, kernel_size=5)\n","\n","# Detect products and mask them iteratively\n","for scene_image, original_image in zip(filtered_scene_images, scene_images_first_part):\n","    result_image, product_detections = detect_and_mask_products(scene_image, model_images_first_part)\n","    final_image = draw_detections_on_image(original_image, product_detections, model_images_first_part)\n","    for detection in product_detections:\n","        best_match, max_matches, position, width, height, dst = detection\n","        print(f\"Product {best_match + 1} {{match: {max_matches}, position: {position}, width: {width}px, height: {height}px}}\")\n","\n","    plt.imshow(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))  # Display the final image with detections\n","    plt.title('Final Image with Detections')\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"CshvP-piZsDx","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mn2rBIdk5UCMobyIDbqlSy_nGghIQtpi"},"executionInfo":{"status":"ok","timestamp":1716560311789,"user_tz":-120,"elapsed":172592,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}},"outputId":"1042e233-640a-4232-a637-f5893e23a536"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["def detect_and_mask_products_taskb(scene_image, model_images):\n","    sift = cv2.SIFT_create()\n","    index_params = dict(algorithm=1, trees=5)\n","    search_params = dict(checks=50)\n","    flann = cv2.FlannBasedMatcher(index_params, search_params)\n","\n","    detections = []\n","    masked_image = scene_image.copy()\n","    scene_height, scene_width = scene_image.shape[:2]\n","\n","    while True:\n","        scene_keypoints, scene_descriptors = sift.detectAndCompute(masked_image, None)\n","        max_matches = 0\n","        best_match = None\n","        best_model_keypoints = None\n","        best_homography = None\n","\n","        for model_idx, model in enumerate(model_images):\n","            model_keypoints, model_descriptors = sift.detectAndCompute(model, None)\n","            matches = flann.knnMatch(model_descriptors, scene_descriptors, k=2)\n","            good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]  # Lowered distance\n","\n","            if len(good_matches) > max_matches:\n","                max_matches = len(good_matches)\n","                best_match = model_idx\n","                best_model_keypoints = model_keypoints\n","\n","                if len(good_matches) > 10:\n","                    src_pts = np.float32([model_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","                    dst_pts = np.float32([scene_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","                    homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","                    best_homography = homography\n","\n","        if max_matches < 10:\n","            break  # Stop if new matches are significantly lower than previous or below the fixed threshold\n","\n","        if best_homography is not None:\n","            # Calculate coordinates of model in scene and mask it\n","            h, w = model_images[best_match].shape[:2]\n","\n","            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n","            dst = cv2.perspectiveTransform(pts, best_homography)\n","\n","\n","            # Calculate angles between consecutive corners\n","            angles = []\n","            num_corners = len(dst)\n","            for i in range(num_corners):\n","                v1 = dst[i][0] - dst[i - 1][0]\n","                v2 = dst[(i + 1) % num_corners][0] - dst[i][0]\n","                angle = angle_between_vectors(v1, v2)\n","                angles.append(angle)\n","\n","            print(angles)\n","            print(max_matches)\n","\n","            if any(60 > angle or angle > 120 for angle in angles):\n","              break\n","\n","\n","            # # Skip detection if detected area is less than half of the reference area or less than half of the last detected area or angles are not close to 90 degrees\n","            # if not all(80 <= angle <= 100 for angle in angles):\n","            #     # Mask the detected area for processing but don't add to detections\n","            #     cv2.fillConvexPoly(masked_image, dst.astype(int), 0)\n","            #     continue\n","\n","            position = tuple(np.int32(dst[0, 0]))\n","            width = int(np.linalg.norm(dst[0][0] - dst[1][0]))\n","            height = int(np.linalg.norm(dst[0][0] - dst[3][0]))\n","\n","            detection_details = (best_match, max_matches, position, width, height, dst)\n","            detections.append(detection_details)\n","\n","            # Mask the detected area\n","            cv2.fillConvexPoly(masked_image, dst.astype(int), 0)\n","\n","    return masked_image, detections\n","\n","def draw_detections_on_image(image, detections, model_images):\n","    for detection in detections:\n","        best_match, max_matches, position, width, height, dst = detection\n","        color = (0, 255, 0)  # Green color for the rectangle\n","        cv2.polylines(image, [np.int32(dst)], True, color, 3)\n","        label = f'Product {best_match}'\n","        cv2.putText(image, label, position, cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n","    return image\n"],"metadata":{"id":"2fC5L4QU_aZ9","executionInfo":{"status":"ok","timestamp":1716566363410,"user_tz":-120,"elapsed":422,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Load and separate images\n","model_images_first_part, model_images_second_part = load_and_split_images(models_folder, 14)\n","scene_images_first_part, scene_images_second_part = load_and_split_images(scenes_folder, 5, cv2.IMREAD_COLOR)\n","\n","# Apply median filter and convert to grayscale\n","filtered_scene_images = apply_median_filter(scene_images_second_part, kernel_size=5)\n","\n","# Detect products and mask them iteratively\n","for scene_image, original_image in zip(filtered_scene_images, scene_images_second_part):\n","    result_image, product_detections = detect_and_mask_products_taskb(scene_image,model_images_second_part)\n","    final_image = draw_detections_on_image(original_image, product_detections, model_images_second_part)\n","    for detection in product_detections:\n","        best_match, max_matches, position, width, height, dst = detection\n","        print(f\"Product {best_match + 16} {{match: {max_matches}, position: {position}, width: {width}px, height: {height}px}}\")\n","\n","    plt.imshow(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))  # Display the final image with detections\n","    plt.title('Final Image with Detections')\n","    plt.axis('off')\n","    plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-okpt9SJaQwXJKdBa2NF3j1sJCUsQhkU"},"id":"nEwoUrfO_Mas","executionInfo":{"status":"ok","timestamp":1716561061601,"user_tz":-120,"elapsed":153877,"user":{"displayName":"Eliott Vigier","userId":"02934055737839168038"}},"outputId":"21134b17-65b0-411d-f270-1984a06ab84c"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["To see a specific image of the database"],"metadata":{"id":"ankYz2mtaqVc"}},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","# scenes_folder = '/content/dataset/scenes'\n","# def apply_median_filter(images, kernel_size=9):\n","#     filtered_images = []\n","#     for img in images:\n","#         filtered_img = cv2.medianBlur(img, kernel_size)\n","#         filtered_images.append(filtered_img)\n","#     return filtered_images\n","\n","# # Load and split images from the scenes folder\n","# scenes_folder = '/content/dataset/scenes'\n","# scene_images_first_part, _ = load_and_split_images(scenes_folder, 5, cv2.IMREAD_COLOR)\n","\n","# # Apply the median filter to the loaded images\n","# filtered_scene_images = apply_median_filter(scene_images_first_part, kernel_size=5)\n","\n","# # Convert a filtered image to grayscale and display it\n","# gray_image = cv2.cvtColor(filtered_scene_images[4], cv2.COLOR_BGR2RGB)\n","# plt.imshow(gray_image)  # Ensure the image is shown in grayscale\n","# plt.axis('off')  # Hide axes for clarity\n","# plt.show()"],"metadata":{"id":"Ahs-wHQsaf2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plt.imshow(model_images_first_part[5], cmap='gray')"],"metadata":{"id":"adou-D-ial80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8ytzALPaKjLb"},"execution_count":null,"outputs":[]}]}