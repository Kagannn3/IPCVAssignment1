{"cells":[{"cell_type":"markdown","metadata":{"id":"MNBgGYg_lpVN"},"source":["# Assignment Module 2: Product Classification\n","\n","The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Assignment Module 2: Product Classification\n","\n","The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preliminaries: the dataset\n","\n","The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n","\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n","</p>\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n","</p>\n","\n","The products belong to the following 43 classes:\n","```\n","0.  Apple\n","1.  Avocado\n","2.  Banana\n","3.  Kiwi\n","4.  Lemon\n","5.  Lime\n","6.  Mango\n","7.  Melon\n","8.  Nectarine\n","9.  Orange\n","10. Papaya\n","11. Passion-Fruit\n","12. Peach\n","13. Pear\n","14. Pineapple\n","15. Plum\n","16. Pomegranate\n","17. Red-Grapefruit\n","18. Satsumas\n","19. Juice\n","20. Milk\n","21. Oatghurt\n","22. Oat-Milk\n","23. Sour-Cream\n","24. Sour-Milk\n","25. Soyghurt\n","26. Soy-Milk\n","27. Yoghurt\n","28. Asparagus\n","29. Aubergine\n","30. Cabbage\n","31. Carrots\n","32. Cucumber\n","33. Garlic\n","34. Ginger\n","35. Leek\n","36. Mushroom\n","37. Onion\n","38. Pepper\n","39. Potato\n","40. Red-Beet\n","41. Tomato\n","42. Zucchini\n","```\n","\n","The dataset is split into training (`train`), validation (`val`), and test (`test`) set."]},{"cell_type":"markdown","metadata":{},"source":["The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning into 'GroceryStoreDataset'...\n","Updating files:  21% (1218/5717)\n","Updating files:  22% (1258/5717)\n","Updating files:  23% (1315/5717)\n","Updating files:  24% (1373/5717)\n","Updating files:  25% (1430/5717)\n","Updating files:  26% (1487/5717)\n","Updating files:  27% (1544/5717)\n","Updating files:  28% (1601/5717)\n","Updating files:  29% (1658/5717)\n","Updating files:  30% (1716/5717)\n","Updating files:  31% (1773/5717)\n","Updating files:  32% (1830/5717)\n","Updating files:  33% (1887/5717)\n","Updating files:  34% (1944/5717)\n","Updating files:  35% (2001/5717)\n","Updating files:  36% (2059/5717)\n","Updating files:  37% (2116/5717)\n","Updating files:  37% (2123/5717)\n","Updating files:  38% (2173/5717)\n","Updating files:  39% (2230/5717)\n","Updating files:  40% (2287/5717)\n","Updating files:  41% (2344/5717)\n","Updating files:  42% (2402/5717)\n","Updating files:  43% (2459/5717)\n","Updating files:  44% (2516/5717)\n","Updating files:  45% (2573/5717)\n","Updating files:  46% (2630/5717)\n","Updating files:  47% (2687/5717)\n","Updating files:  48% (2745/5717)\n","Updating files:  49% (2802/5717)\n","Updating files:  50% (2859/5717)\n","Updating files:  51% (2916/5717)\n","Updating files:  52% (2973/5717)\n","Updating files:  53% (3031/5717)\n","Updating files:  53% (3036/5717)\n","Updating files:  54% (3088/5717)\n","Updating files:  55% (3145/5717)\n","Updating files:  56% (3202/5717)\n","Updating files:  57% (3259/5717)\n","Updating files:  58% (3316/5717)\n","Updating files:  59% (3374/5717)\n","Updating files:  60% (3431/5717)\n","Updating files:  61% (3488/5717)\n","Updating files:  62% (3545/5717)\n","Updating files:  63% (3602/5717)\n","Updating files:  64% (3659/5717)\n","Updating files:  64% (3674/5717)\n","Updating files:  65% (3717/5717)\n","Updating files:  66% (3774/5717)\n","Updating files:  67% (3831/5717)\n","Updating files:  68% (3888/5717)\n","Updating files:  69% (3945/5717)\n","Updating files:  70% (4002/5717)\n","Updating files:  71% (4060/5717)\n","Updating files:  72% (4117/5717)\n","Updating files:  73% (4174/5717)\n","Updating files:  74% (4231/5717)\n","Updating files:  75% (4288/5717)\n","Updating files:  76% (4345/5717)\n","Updating files:  77% (4403/5717)\n","Updating files:  78% (4460/5717)\n","Updating files:  79% (4517/5717)\n","Updating files:  79% (4551/5717)\n","Updating files:  80% (4574/5717)\n","Updating files:  81% (4631/5717)\n","Updating files:  82% (4688/5717)\n","Updating files:  83% (4746/5717)\n","Updating files:  84% (4803/5717)\n","Updating files:  85% (4860/5717)\n","Updating files:  86% (4917/5717)\n","Updating files:  87% (4974/5717)\n","Updating files:  88% (5031/5717)\n","Updating files:  89% (5089/5717)\n","Updating files:  90% (5146/5717)\n","Updating files:  91% (5203/5717)\n","Updating files:  92% (5260/5717)\n","Updating files:  93% (5317/5717)\n","Updating files:  94% (5374/5717)\n","Updating files:  95% (5432/5717)\n","Updating files:  95% (5437/5717)\n","Updating files:  96% (5489/5717)\n","Updating files:  97% (5546/5717)\n","Updating files:  98% (5603/5717)\n","Updating files:  99% (5660/5717)\n","Updating files: 100% (5717/5717)\n","Updating files: 100% (5717/5717), done.\n"]}],"source":["\n","!git clone https://github.com/marcusklasson/GroceryStoreDataset.git "]},{"cell_type":"markdown","metadata":{},"source":["This command is used to create GroceryStoreDataset named directory in the current working directory\n","It contains all dataset files "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cpu\n"]}],"source":["from pathlib import Path # class for handling file paths \n","from PIL import Image # class for opening and manipulating images from the Python imaging library \n","from typing import List, Tuple # typing library provides about typing hints for code readability and error checking\n","\n","import torch \n","from torch import Tensor\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import Adam, SGD, AdamW\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torchvision\n","# Torch and TorchVision libraries are significant libraries for building and training neural networks.\n","\n","from tqdm.notebook import tqdm\n","# tqdm is a library that provides a progress bar for loops and other iterable objects. It is useful\n","import matplotlib.pyplot as plt\n","# matplotlib is a library for creating static, animated, and interactive visualizations in Python.\n","import numpy as np\n","# used for numerical operations \n","\n","\n","import random\n","\n","\n","# Check device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# Check if GPU is available and use it if it is. Otherwise, use the CPU.\n","print(f\"Device: {device}\")\n","\n","# Fix random seed\n","def fix_random(seed: int) -> None:  \n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # \n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","fix_random(45)\n","\n","# Define the GroceryStoreDataset class\n","class GroceryStoreDataset(Dataset):\n","    def __init__(self, split: str, transform=None) -> None:\n","        super().__init__()\n","        self.root = Path(\"GroceryStoreDataset/dataset\")\n","        self.split = split\n","        self.paths, self.labels = self.read_file()\n","        self.transform = transform\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx) -> Tuple[torch.Tensor, int]:\n","        img = Image.open(self.root / self.paths[idx])\n","        label = self.labels[idx]\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, label\n","\n","    def read_file(self) -> Tuple[List[str], List[int]]:\n","        paths, labels = [], []\n","        with open(self.root / f\"{self.split}.txt\") as f:\n","            for line in f:\n","                path, _, label = line.strip().split(\", \")\n","                paths.append(path)\n","                labels.append(int(label))\n","        return paths, labels\n","\n","    def get_num_classes(self) -> int:\n","        return max(self.labels) + 1\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Initial transform\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Load datasets\n","train_dataset = GroceryStoreDataset(split='train', transform=transform)\n","val_dataset = GroceryStoreDataset(split='val', transform=transform)\n","test_dataset = GroceryStoreDataset(split='test', transform=transform)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["464 464\n"]}],"source":["# Checking image shapes\n","def check_image_shapes(dataset):\n","    w, h = 0, 0\n","    for i in range(len(dataset)):\n","        img, _ = dataset[i]\n","        if img.shape[1] > w:\n","            w = img.shape[1]\n","        if img.shape[2] > h:\n","            h = img.shape[2]\n","    return w, h\n","\n","w_t, h_t = check_image_shapes(train_dataset)\n","print(w_t, h_t)  # Example output: 464, 464"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from torchvision import transforms\n","\n","# Define the transform to resize images\n","image_height, image_width, image_channels = 348, 348, 3\n","resize_transform = transforms.Compose([\n","    transforms.Resize((image_height, image_width)),\n","    transforms.ToTensor()\n","])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Load the training dataset with the resizing transform\n","train_dataset = GroceryStoreDataset(split='train', transform=resize_transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean: tensor([0.5306, 0.3964, 0.2564])\n","Std: tensor([0.2325, 0.2093, 0.1781])\n"]}],"source":["# Compute mean and standard deviation\n","mean = torch.zeros(3)\n","std = torch.zeros(3)\n","nb_samples = 0\n","\n","for images, _ in train_loader:\n","    batch_samples = images.size(0)\n","    images = images.view(batch_samples, images.size(1), -1)\n","    mean += images.mean(2).sum(0)\n","    std += images.std(2).sum(0)\n","    nb_samples += batch_samples\n","\n","mean /= nb_samples\n","std /= nb_samples\n","\n","print(f'Mean: {mean}')\n","print(f'Std: {std}')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Define accuracy and training loop functions\n","def ncorrect(scores, y):\n","    y_hat = torch.argmax(scores, -1)\n","    return (y_hat == y).sum()\n","\n","def accuracy(scores, y):\n","    correct = ncorrect(scores, y)\n","    return correct.true_divide(y.shape[0])\n","\n","def train_loop(model, train_dl, epochs, opt, val_dl=None, verbose=False, label_smoothing=0):\n","    best_val_acc = 0\n","    best_params = []\n","    best_epoch = -1\n","    for e in tqdm(range(epochs)):\n","        model.train()\n","        train_loss, train_samples, train_acc = 0, 0, 0\n","        for train_data in train_dl:\n","            imgs = train_data[0].to(device)\n","            labels = train_data[1].to(device)\n","            scores = model(imgs)\n","            loss = F.cross_entropy(scores, labels, label_smoothing=label_smoothing)\n","            train_loss += loss.item() * imgs.shape[0]\n","            train_samples += imgs.shape[0]\n","            train_acc += ncorrect(scores, labels).item()\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","        train_acc /= train_samples\n","        train_loss /= train_samples\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_samples, val_acc = 0, 0, 0\n","            if val_dl is not None:\n","                for val_data in val_dl:\n","                    imgs = val_data[0].to(device)\n","                    labels = val_data[1].to(device)\n","                    val_scores = model(imgs)\n","                    val_loss += F.cross_entropy(val_scores, labels).item() * imgs.shape[0]\n","                    val_samples += imgs.shape[0]\n","                    val_acc += ncorrect(val_scores, labels).item()\n","                val_acc /= val_samples\n","                val_loss /= val_samples\n","                if val_acc > best_val_acc:\n","                    best_val_acc = val_acc\n","                    best_params = model.state_dict()\n","                    torch.save(best_params, \"best_model.pth\")\n","                    best_epoch = e\n","\n","        if verbose:\n","            print(f\"Epoch {e}: train loss {train_loss:.3f} - train acc {train_acc:.3f}\" + (\"\" if val_dl is None else f\" - valid loss {val_loss:.3f} - valid acc {val_acc:.3f}\"))\n","            if val_acc >= 0.60:\n","                best_params = model.state_dict()\n","                torch.save(best_params, \"A2.pth\")\n","                best_epoch = e\n","                break\n","\n","    if verbose and val_dl is not None:\n","        print(f\"Best epoch {best_epoch}, best acc {best_val_acc}\")\n","\n","    return best_val_acc, best_params, best_epoch"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Define the CNN\n","class ExtendedCNN(nn.Module):\n","    def __init__(self, channels=16, num_classes=10, p_dropout=0):\n","        super(ExtendedCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels*2, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=channels*2, out_channels=channels*4, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=channels*4, out_channels=channels*8, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        def conv2d_size_out(size, kernel_size=3, stride=1, padding=1):\n","            return (size - kernel_size + 2 * padding) // stride + 1\n","        def conv2d_compute_size(conv_number, kernels, max_poolings):\n","            image_w = image_width\n","            for i in range(conv_number):\n","                image_w = conv2d_size_out(image_w, kernel_size=kernels[i])\n","                if max_poolings[i] == 1:\n","                    image_w = image_w // 2\n","            return image_w\n","        image_w = conv2d_compute_size(4, [3, 3, 3, 3], [1, 1, 1, 1])\n","        linear_input_size = image_w * image_w * channels * 8\n","        self.fc1 = nn.Linear(linear_input_size, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Hyperparameters and DataLoader\n","transform_2 = torchvision.transforms.Compose([\n","    torchvision.transforms.RandomResizedCrop(size=(image_height, image_width), antialias=True),\n","    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean, std)\n","])\n","\n","val_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((image_height, image_width)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean, std)\n","])\n","\n","train_dataset = GroceryStoreDataset(split='train', transform=transform_2)\n","val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n","\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03529af94a96485f8a84d9a46be3d8d6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/80 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 0: train loss 3.160 - train acc 0.186 - valid loss 2.925 - valid acc 0.220\n","Epoch 1: train loss 2.706 - train acc 0.293 - valid loss 2.571 - valid acc 0.304\n","Epoch 2: train loss 2.379 - train acc 0.392 - valid loss 2.309 - valid acc 0.341\n","Epoch 3: train loss 2.191 - train acc 0.441 - valid loss 2.170 - valid acc 0.375\n","Epoch 4: train loss 2.025 - train acc 0.503 - valid loss 2.079 - valid acc 0.365\n","Epoch 5: train loss 1.930 - train acc 0.541 - valid loss 2.205 - valid acc 0.372\n","Epoch 6: train loss 1.816 - train acc 0.578 - valid loss 1.974 - valid acc 0.395\n","Epoch 7: train loss 1.776 - train acc 0.591 - valid loss 2.025 - valid acc 0.368\n","Epoch 8: train loss 1.662 - train acc 0.650 - valid loss 2.147 - valid acc 0.341\n","Epoch 9: train loss 1.667 - train acc 0.646 - valid loss 1.943 - valid acc 0.426\n","Epoch 10: train loss 1.611 - train acc 0.661 - valid loss 2.204 - valid acc 0.382\n","Epoch 11: train loss 1.551 - train acc 0.690 - valid loss 2.273 - valid acc 0.436\n","Epoch 12: train loss 1.514 - train acc 0.714 - valid loss 1.710 - valid acc 0.480\n","Epoch 13: train loss 1.465 - train acc 0.730 - valid loss 2.031 - valid acc 0.453\n","Epoch 14: train loss 1.431 - train acc 0.748 - valid loss 1.996 - valid acc 0.456\n","Epoch 15: train loss 1.432 - train acc 0.745 - valid loss 2.029 - valid acc 0.426\n","Epoch 16: train loss 1.414 - train acc 0.756 - valid loss 1.900 - valid acc 0.500\n","Epoch 17: train loss 1.394 - train acc 0.764 - valid loss 1.974 - valid acc 0.456\n","Epoch 18: train loss 1.350 - train acc 0.783 - valid loss 1.749 - valid acc 0.537\n","Epoch 19: train loss 1.358 - train acc 0.783 - valid loss 1.852 - valid acc 0.503\n","Epoch 20: train loss 1.321 - train acc 0.791 - valid loss 1.941 - valid acc 0.490\n","Epoch 21: train loss 1.320 - train acc 0.803 - valid loss 1.955 - valid acc 0.459\n","Epoch 22: train loss 1.310 - train acc 0.799 - valid loss 1.962 - valid acc 0.476\n","Epoch 23: train loss 1.263 - train acc 0.814 - valid loss 2.053 - valid acc 0.443\n","Epoch 24: train loss 1.275 - train acc 0.809 - valid loss 2.085 - valid acc 0.426\n"]}],"source":["# Model training\n","new_model = ExtendedCNN(num_classes=train_dataset.get_num_classes())\n","new_model.to(device)\n","\n","optimizer = Adam(new_model.parameters(), lr=0.001)\n","epochs = 80\n","label_smoothing = 0.1\n","\n","best_val_acc, best_params, best_epoch = train_loop(\n","    new_model,\n","    train_loader,\n","    epochs,\n","    optimizer,\n","    val_loader,\n","    verbose=True,\n","    label_smoothing=label_smoothing\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Grid Search\n","weight_decays = [0.0, 0.01, 0.001]\n","label_smoothings = [0.0, 0.05, 0.1, 0.15, 0.2]\n","lrs = [0.001]\n","channels = [16]\n","batches = [8]\n","\n","epochs = 80\n","test_iteration = 1\n","total_tests = len(weight_decays) * len(label_smoothings) * len(lrs) * len(channels) * len(batches)\n","\n","for weight_decay in weight_decays:\n","    for label_smoothing in label_smoothings:\n","        for lr in lrs:\n","            for channel in channels:\n","                for batch in batches:\n","                    model = ExtendedCNN(channels=channel, num_classes=train_dataset.get_num_classes())\n","                    model.to(device)\n","                    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","                    print(f\"Training with: weight_decay={weight_decay}, label_smoothing={label_smoothing}, lr={lr}, channels={channel}, batch_size={batch}, test={test_iteration}/{total_tests}\")\n","                    test_iteration += 1\n","                    best_val_acc, best_params, best_epoch = train_loop(\n","                        model,\n","                        DataLoader(train_dataset, batch_size=batch, shuffle=True),\n","                        epochs,\n","                        optimizer,\n","                        DataLoader(val_dataset, batch_size=batch, shuffle=False),\n","                        verbose=True,\n","                        label_smoothing=label_smoothing\n","                    )\n","                    print(f\"Finished training with best_val_acc={best_val_acc:.3f} at epoch {best_epoch}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing on the test dataset\n","test_dataset = GroceryStoreDataset(split='test', transform=val_transforms)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the best model\n","best_model = ExtendedCNN(num_classes=train_dataset.get_num_classes())\n","best_model.load_state_dict(torch.load(\"best_model.pth\"))\n","best_model.to(device)\n","best_model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluate the model\n","test_acc = 0\n","test_samples = 0\n","\n","with torch.no_grad():\n","    for test_data in test_loader:\n","        imgs = test_data[0].to(device)\n","        labels = test_data[1].to(device)\n","        scores = best_model(imgs)\n","        test_acc += ncorrect(scores, labels).item()\n","        test_samples += imgs.size(0)\n","\n","test_acc /= test_samples\n","print(f\"Test accuracy: {test_acc:.3f}\")\n","\n","# Example usage of the best model\n","def predict(image_path):\n","    image = Image.open(image_path)\n","    image = val_transforms(image).unsqueeze(0).to(device)\n","    best_model.eval()\n","    with torch.no_grad():\n","        scores = best_model(image)\n","        predicted_label = torch.argmax(scores, -1).item()\n","    return predicted_label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict an example\n","example_image_path = \"GroceryStoreDataset/dataset/example.jpg\"  # Replace with an actual image path from your dataset\n","predicted_label = predict(example_image_path)\n","print(f\"Predicted label for the example image: {predicted_label}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1: design your own network\n","\n","Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n","\n","- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n","\n","- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n","\n","Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: fine-tune an existing network\n","\n","Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n","\n","1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n","1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."]},{"cell_type":"markdown","metadata":{"id":"dVTQUJ4uYH1w"},"source":["## Preliminaries: the dataset\n","\n","The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n","\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n","</p>\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n","</p>\n","\n","The products belong to the following 43 classes:\n","```\n","0.  Apple\n","1.  Avocado\n","2.  Banana\n","3.  Kiwi\n","4.  Lemon\n","5.  Lime\n","6.  Mango\n","7.  Melon\n","8.  Nectarine\n","9.  Orange\n","10. Papaya\n","11. Passion-Fruit\n","12. Peach\n","13. Pear\n","14. Pineapple\n","15. Plum\n","16. Pomegranate\n","17. Red-Grapefruit\n","18. Satsumas\n","19. Juice\n","20. Milk\n","21. Oatghurt\n","22. Oat-Milk\n","23. Sour-Cream\n","24. Sour-Milk\n","25. Soyghurt\n","26. Soy-Milk\n","27. Yoghurt\n","28. Asparagus\n","29. Aubergine\n","30. Cabbage\n","31. Carrots\n","32. Cucumber\n","33. Garlic\n","34. Ginger\n","35. Leek\n","36. Mushroom\n","37. Onion\n","38. Pepper\n","39. Potato\n","40. Red-Beet\n","41. Tomato\n","42. Zucchini\n","```\n","\n","The dataset is split into training (`train`), validation (`val`), and test (`test`) set."]},{"cell_type":"markdown","metadata":{"id":"1pdrmJRnJPd8"},"source":["The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."]},{"cell_type":"markdown","metadata":{"id":"yBch3dpwNSsW"},"source":["## Part 1: design your own network\n","\n","Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n","\n","- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n","\n","- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n","\n","Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."]},{"cell_type":"markdown","metadata":{"id":"gkWEqSPoUIL3"},"source":["## Part 2: fine-tune an existing network\n","\n","Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n","\n","1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n","1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
